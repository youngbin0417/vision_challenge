{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bvb09\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm # ë¡œì»¬ì—ì„œëŠ” tqdm.notebook ëŒ€ì‹  ì¼ë°˜ tqdm ì‚¬ìš©\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "# skimage\n",
    "from skimage.exposure import rescale_intensity, equalize_hist\n",
    "from skimage.filters import gaussian\n",
    "from skimage.restoration import denoise_bilateral\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops\n",
    "from skimage.transform import resize\n",
    "from scipy import ndimage\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, confusion_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# joblib (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# faiss (KNN ê°€ì†í™”ë¥¼ ìœ„í•´)\n",
    "import faiss\n",
    "\n",
    "# ë°ì´í„° ê´€ë ¨ \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensemble\n",
    "from Ensemble import EnsembleFaissKNN\n",
    "\n",
    "#DATASET_BASE_PATH = \"C:/Users/bvb09/.cache/kagglehub/datasets/mikhailma/test-dataset/versions/1/Google_Recaptcha_V2_Images_Dataset\"\n",
    "\n",
    "print(\"âœ” ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "# ==============================================================================\n",
    "\n",
    "# ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜\n",
    "def load_images_from_folder(base_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_folder_path = os.path.join(base_path, 'images')\n",
    "    \n",
    "    if not os.path.exists(image_folder_path):\n",
    "        raise FileNotFoundError(f\"ì´ë¯¸ì§€ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_folder_path}\\n\"\n",
    "                                f\"DATASET_BASE_PATHë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    for label_name in tqdm(os.listdir(image_folder_path), desc=\"í´ë” ë¡œë“œ ì¤‘\"):\n",
    "        label_path = os.path.join(image_folder_path, label_name)\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_name in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_name)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        images.append(img)\n",
    "                        labels.append(label_name)\n",
    "                    else:\n",
    "                        print(f\"ê²½ê³ : {img_path} ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ê²½ê³ : {img_path} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}\")\n",
    "    return pd.DataFrame({'image_data': images, 'label': labels})\n",
    "\n",
    "def visualize_features(X_feats, y_labels, method='pca'):\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    else:\n",
    "        from sklearn.manifold import TSNE\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "    X_reduced = reducer.fit_transform(X_feats)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_labels, cmap='tab20', s=10, alpha=0.7)\n",
    "    plt.title(f'Feature Distribution via {method.upper()}')\n",
    "    plt.colorbar(scatter, ticks=range(len(set(y_labels))))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge(img_bgr):\n",
    "    # 1) ê°•ì œ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    img_bgr = cv2.resize(img_bgr, (120, 120), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 2) CLAHE â†’ Gray â†’ Blur â†’ Denoise â†’ Canny\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(2.0, (8,8))\n",
    "    l = clahe.apply(l)\n",
    "    img_eq = cv2.cvtColor(cv2.merge((l,a,b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    gray = cv2.cvtColor(img_eq, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    denoised = cv2.fastNlMeansDenoising(blurred, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "    edges = cv2.Canny(denoised, 100, 230)\n",
    "\n",
    "    #plt.imshow(edges, cmap='gray')\n",
    "    #plt.title(\"Preprocessed\")\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "    return edges  # í•­ìƒ (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# 1) ê³µí†µ: BGR â†’ LAB â†’ CLAHE(L) â†’ BGR (í•„ìš” ì‹œ)\n",
    "def apply_clahe(img_bgr):\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l = cv2.createCLAHE(2.0, (8,8)).apply(l)\n",
    "    return cv2.cvtColor(cv2.merge((l,a,b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# 3) Unsharp mask (SIFT ì „ìš©)\n",
    "def unsharp(img_gray):\n",
    "    blurred = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "    return cv2.addWeighted(img_gray, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "# 4) Mild Gaussian blur (LBP/GLCM/Laws)\n",
    "def mild_blur(img_gray):\n",
    "    return cv2.GaussianBlur(img_gray, (3,3), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(img):\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkcol(img_bgr):\n",
    "    #image = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    #image=cv2.GaussianBlur(image, ksize=(3,3), sigmaX=0)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    #image = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, (8,8,8), [0,180, 0,256, 0,256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    hist_rgb = cv2.calcHist([rgb], [0, 1, 2], None, (8,8,8), [0,256, 0,256, 0,256])\n",
    "    hist_rgb = cv2.normalize(hist_rgb, hist_rgb).flatten()\n",
    "    hist_bgr = cv2.calcHist([img_bgr], [0, 1, 2], None, (8,8,8), [0,256, 0,256, 0,256])\n",
    "    hist_bgr = cv2.normalize(hist_bgr, hist_bgr).flatten()\n",
    "    #plt.imshow(v)\n",
    "    #plt.title(\"Color\")\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "     # âœ… 2D ì‹œê°í™”\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(hist, color='blue')\n",
    "    plt.title(\"Flattened HSV Histogram\")\n",
    "    plt.xlabel(\"Bin Index\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return hsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í´ë” ë¡œë“œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 124.96it/s]\n",
      "ì´ë¯¸ì§€ ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 24.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tst = \"C:/Users/bvb09/.cache/kagglehub/datasets/pre\"\n",
    "timg = load_images_from_folder(tst)\n",
    "timg['processed_image_data'] = [edge(img) for img in tqdm(timg['image_data'], desc=\"ì´ë¯¸ì§€ ì „ì²˜ë¦¬\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  í”¼ì³ ì¶”ì¶œ í•¨ìˆ˜\n",
    "# ==================================================================:============\n",
    "\n",
    "def extract_color_histogram_features(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0,1,2], None, (8,8,8), [0,180,0,256,0,256])\n",
    "    return cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "def extract_sift_pca_mean(img_bgr, pca_model=None, n_components=32):\n",
    "    # 1. BGR â†’ GRAY\n",
    "    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 2. SIFT ìƒì„± ë° ë””ìŠ¤í¬ë¦½í„° ì¶”ì¶œ\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_gray, None)\n",
    "\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        # ë””ìŠ¤í¬ë¦½í„° ì—†ì„ ê²½ìš° 0 ë²¡í„° ë°˜í™˜\n",
    "        return np.zeros(n_components if pca_model else 128, dtype=np.float32)\n",
    "\n",
    "    # 3. PCA ì²˜ë¦¬\n",
    "    if pca_model is None:\n",
    "        # PCA í•™ìŠµë„ í¬í•¨ (ë³´í†µ í•™ìŠµì…‹ì—ì„œ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ê²Œ ì¢‹ìŒ)\n",
    "        pca_model = PCA(n_components=n_components)\n",
    "        descriptors_pca = pca_model.fit_transform(descriptors)\n",
    "    else:\n",
    "        descriptors_pca = pca_model.transform(descriptors)\n",
    "\n",
    "    # 4. í‰ê·  ë²¡í„° ë°˜í™˜\n",
    "    mean_vector = np.mean(descriptors_pca, axis=0)\n",
    "    return mean_vector.astype(np.float32)\n",
    "\n",
    "def extract_glcm_features(image):\n",
    "    if image is None:\n",
    "        num_props = 6\n",
    "        num_distances = 3 # ì•„ë˜ distances ë¦¬ìŠ¤íŠ¸ ê¸¸ì´\n",
    "        num_angles = 4    # ì•„ë˜ angles ë¦¬ìŠ¤íŠ¸ ê¸¸ì´\n",
    "        return np.zeros(num_props * num_distances * num_angles)\n",
    "    \n",
    "    img_glcm = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_glcm = mild_blur(img_glcm)\n",
    "    img_glcm = apply_clahe(cv2.cvtColor(img_glcm, cv2.COLOR_GRAY2BGR))\n",
    "    gray_image = cv2.cvtColor(img_glcm, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    distances = [1, 2, 3] # ì˜ˆì‹œ ê±°ë¦¬ ê°’\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4] # ì˜ˆì‹œ ê°ë„ ê°’\n",
    "    \n",
    "    try:\n",
    "        glcm = graycomatrix(gray_image, distances=distances, angles=angles, symmetric=True, normed=True)\n",
    "        \n",
    "        props_to_extract = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "        glcm_features = []\n",
    "        for prop in props_to_extract:\n",
    "            glcm_features.append(graycoprops(glcm, prop).ravel())\n",
    "            \n",
    "        return np.concatenate(glcm_features)\n",
    "    except Exception as e:\n",
    "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ë¦¬ (ì˜ˆ: 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ ë°˜í™˜)\n",
    "        print(f\"GLCM ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        num_props = 6\n",
    "        num_distances = len(distances) \n",
    "        num_angles = len(angles)    \n",
    "        return np.zeros(num_props * num_distances * num_angles)\n",
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2)):\n",
    "     # 1) ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    img = cv2.resize(image, (120, 120), interpolation=cv2.INTER_AREA)\n",
    "    # 2) ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    features = hog(gray,\n",
    "                   orientations=orientations,\n",
    "                   pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block,\n",
    "                   block_norm='L2-Hys',\n",
    "                   visualize=False,\n",
    "                   transform_sqrt=True,\n",
    "                   feature_vector=True)\n",
    "    return features.astype(np.float32)\n",
    "\n",
    "def extract_sift_descriptors_from_array(image):\n",
    "    img = apply_clahe(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image=unsharp(img)\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray_image, None) # í‚¤í¬ì¸íŠ¸ì™€ ë””ìŠ¤í¬ë¦½í„° ê³„ì‚°\n",
    "\n",
    "    return des # ë””ìŠ¤í¬ë¦½í„°ê°€ ì—†ì„ ê²½ìš° None ë°˜í™˜\n",
    "\n",
    "def extract_lbp_features_from_array(image, P=8, R=1, method='uniform'):\n",
    "    \n",
    "    image=apply_clahe(image)\n",
    "    #image=mild_blur(image)\n",
    "    gray_image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # local_binary_pattern í•¨ìˆ˜ëŠ” float íƒ€ì… ì´ë¯¸ì§€ë¥¼ ì„ í˜¸í•˜ì§€ë§Œ, skimageëŠ” UBYTEë„ ì²˜ë¦¬\n",
    "    lbp_image = local_binary_pattern(gray_image, P, R, method=method)\n",
    "\n",
    "    # LBP íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°\n",
    "    max_bins = P * (P - 1) + 3 if method == 'default' else P + 2 # uniformì˜ ê²½ìš° P+2\n",
    "    hist, _ = np.histogram(lbp_image.ravel(), bins=max_bins, range=(0, max_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Laws' Texture Energy - ê¸°ì¡´ê³¼ ë™ì¼\n",
    "def extract_laws_energy_features(image, window_size=15):\n",
    "    image_gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_gray=mild_blur(image_gray)\n",
    "\n",
    "    L5 = np.array([1, 4, 6, 4, 1], dtype=np.float32)\n",
    "    E5 = np.array([-1, -2, 0, 2, 1], dtype=np.float32)\n",
    "    S5 = np.array([-1, 0, 2, 0, -1], dtype=np.float32)\n",
    "    W5 = np.array([-1, 2, 0, -2, 1], dtype=np.float32)\n",
    "    R5 = np.array([1, -4, 6, -4, 1], dtype=np.float32)\n",
    "    kernels = [L5, E5, S5, W5, R5]\n",
    "\n",
    "    energy_features = []\n",
    "    if image_gray.dtype == np.uint8:\n",
    "        image_gray = image_gray.astype(np.float32)\n",
    "\n",
    "    for k1 in kernels:\n",
    "        for k2 in kernels:\n",
    "            kernel = np.outer(k1, k2)\n",
    "            filtered = ndimage.convolve(image_gray, kernel, mode='reflect')\n",
    "            energy = np.abs(filtered)\n",
    "            summed = cv2.boxFilter(energy, ddepth=-1, ksize=(window_size, window_size), normalize=False)\n",
    "            energy_features.append(summed.mean())\n",
    "\n",
    "    return np.array(energy_features, dtype=np.float32)\n",
    "\n",
    "def learn_bovw_vocabulary(all_sift_descriptors, num_clusters=200):\n",
    "    \"\"\"\n",
    "    Bag of Visual Words (BoVW)ë¥¼ ìœ„í•œ ì‹œê°ì  ë‹¨ì–´(Vocabulary)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    filtered = [des for des in all_sift_descriptors if des is not None and len(des) > 0]\n",
    "\n",
    "    # âœ” ì°¨ì› í™•ì¸: ëª¨ë“  ë””ìŠ¤í¬ë¦½í„°ì˜ feature ì°¨ì›ì´ ê°™ì•„ì•¼ í•¨\n",
    "    feature_dims = {des.shape[1] for des in filtered}\n",
    "    if len(feature_dims) > 1:\n",
    "        raise ValueError(f\"âŒ BoVW í•™ìŠµìš© SIFT ë””ìŠ¤í¬ë¦½í„°ë“¤ì˜ ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ: {feature_dims}\")\n",
    "\n",
    "    concatenated_descriptors = np.vstack(filtered)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    _, _, centers = cv2.kmeans(\n",
    "        concatenated_descriptors.astype(np.float32),\n",
    "        num_clusters, None, criteria, 10, flags\n",
    "    )\n",
    "    return centers\n",
    "\n",
    "\n",
    "def create_bovw_histogram(sift_descriptors, vocabulary):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ ì´ë¯¸ì§€ì˜ SIFT â†’ BoVW íˆìŠ¤í† ê·¸ë¨ (ë²¡í„°í™” ë²„ì „)\n",
    "    \"\"\"\n",
    "    num_clusters = vocabulary.shape[0]\n",
    "    if sift_descriptors is None or len(sift_descriptors) == 0:\n",
    "        return np.zeros(num_clusters, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        # âœ” ìœ í´ë¦¬ë””ì–¸ ê±°ë¦¬ ê³„ì‚° (ë²¡í„°í™”)\n",
    "        distances = np.linalg.norm(\n",
    "            vocabulary[None, :, :] - sift_descriptors[:, None, :], axis=2\n",
    "        )  # shape: (num_des, num_clusters)\n",
    "\n",
    "        closest_clusters = np.argmin(distances, axis=1)\n",
    "        histogram = np.bincount(closest_clusters, minlength=num_clusters).astype(np.float32)\n",
    "        histogram = cv2.normalize(histogram, None, norm_type=cv2.NORM_L2).flatten()\n",
    "        return histogram\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ BoVW íˆìŠ¤í† ê·¸ë¨ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "        return np.zeros(num_clusters, dtype=np.float32)\n",
    "\n",
    "\n",
    "def parallel_create_bovw_histograms(descriptor_list, vocabulary, n_jobs=6):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ SIFT â†’ BoVW íˆìŠ¤í† ê·¸ë¨ (ë©€í‹°í”„ë¡œì„¸ì‹±)\n",
    "    \"\"\"\n",
    "    histograms = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(create_bovw_histogram)(desc, vocabulary) for desc in descriptor_list\n",
    "    )\n",
    "    return np.array(histograms, dtype=np.float32)\n",
    "\n",
    "print(\"âœ” íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_split(df, image_dir, bovw_vocabulary=None, num_bovw_clusters=200):\n",
    "\n",
    "    hog_feats = []\n",
    "    color_feats = []\n",
    "    lbp_feats = []\n",
    "    sift_descriptors_raw = [] # SIFT ë””ìŠ¤í¬ë¦½í„°ë§Œ\n",
    "\n",
    "    dataset_name = df.name if hasattr(df, 'name') else 'dataset'\n",
    "    print(f\"Loading images and extracting raw features for {dataset_name}...\")\n",
    "\n",
    "    # ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆ ë¡œë“œí•˜ê³ , ì´ë¥¼ ê° íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ì— ì „ë‹¬\n",
    "    for img_name in tqdm(df['image_name'], desc=f\"Processing images for {dataset_name}\"):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "\n",
    "        # ëª¨ë“  ì´ë¯¸ì§€ê°€ ì´ë¯¸ 120x120ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ, ì¶”ê°€ ë¦¬ì‚¬ì´ì¦ˆ ì—†ìŒ\n",
    "        # ë‹¤ë§Œ, cv2.imreadëŠ” BGRë¡œ ì½ìœ¼ë¯€ë¡œ, ì»¬ëŸ¬ íˆìŠ¤í† ê·¸ë¨ì„ ìœ„í•´ RGB ë³€í™˜ì€ í•„ìš”\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # RGB ë³€í™˜ì€ ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ìˆ˜í–‰\n",
    "\n",
    "        # ê° íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ì— ì´ë¯¸ì§€ ë°°ì—´ ì „ë‹¬\n",
    "        hog_feats.append(extract_hog_features(image_rgb)) # HOGëŠ” ë‚´ë¶€ì—ì„œ GRAY ë³€í™˜\n",
    "        color_feats.append(extract_color_histogram_features(image_rgb))\n",
    "        lbp_feats.append(extract_lbp_features_from_array(image_rgb)) # LBPëŠ” ë‚´ë¶€ì—ì„œ GRAY ë³€í™˜\n",
    "        sift_descriptors_raw.append(extract_sift_descriptors_from_array(image_rgb)) # SIFTëŠ” ë‚´ë¶€ì—ì„œ GRAY ë³€í™˜\n",
    "\n",
    "\n",
    "    current_bovw_vocab = bovw_vocabulary\n",
    "    if current_bovw_vocab is None: # í•™ìŠµ ë°ì´í„°ì—ì„œë§Œ Vocabulary í•™ìŠµ\n",
    "        print(f\"Learning BoVW vocabulary for {dataset_name}...\")\n",
    "        current_bovw_vocab = learn_bovw_vocabulary(sift_descriptors_raw, num_clusters=num_bovw_clusters)\n",
    "        print(f\"BoVW vocabulary learned with {current_bovw_vocab.shape[0]} clusters.\")\n",
    "\n",
    "    all_bovw_feats = []\n",
    "    print(f\"Creating BoVW histograms for {dataset_name}...\")\n",
    "    for sift_des in tqdm(sift_descriptors_raw):\n",
    "        all_bovw_feats.append(create_bovw_histogram(sift_des, current_bovw_vocab))\n",
    "\n",
    "    return (np.array(hog_feats), np.array(color_feats),\n",
    "            np.array(all_bovw_feats), np.array(lbp_feats), current_bovw_vocab)\n",
    "\n",
    "\n",
    "def combine_features(*feature_arrays):\n",
    "    reshaped = []\n",
    "    for arr in feature_arrays:\n",
    "        arr = np.asarray(arr)\n",
    "        if arr.ndim == 3:\n",
    "            # (N, H, W) â†’ (N, H*W)\n",
    "            arr = arr.reshape(arr.shape[0], -1)\n",
    "        elif arr.ndim == 1:\n",
    "            # (D,) â†’ (1, D)\n",
    "            arr = arr.reshape(1, -1)\n",
    "        reshaped.append(arr)\n",
    "    return np.hstack(reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  ë¶„ë¥˜ ëª¨ë¸ ë° í•™ìŠµ/í‰ê°€ í•¨ìˆ˜\n",
    "# ==============================================================================\n",
    "# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ Faiss KNN í•™ìŠµ (ìˆ˜ì •ë¨)\n",
    "def train_faiss_knn_euclidean(X_train, y_train, n_neighbors=3):\n",
    "    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ì—ì„œëŠ” ë°ì´í„° ì •ê·œí™”ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n",
    "    # X_train = normalize(X_train, axis=1) # ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    D = X_train.shape[1]\n",
    "\n",
    "    # Inner Product (IP) ëŒ€ì‹  L2 ê±°ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    index = faiss.IndexFlatL2(D) # <-- ë³€ê²½ëœ ë¶€ë¶„\n",
    "    index.add(X_train)\n",
    "\n",
    "    return index, y_train, n_neighbors\n",
    "\n",
    "# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ Faiss KNN ì˜ˆì¸¡ (ìˆ˜ì •ë¨)\n",
    "def predict_faiss_knn_euclidean(index, y_train_labels, n_neighbors, X_test):\n",
    "    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ê·œí™”ë„ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n",
    "    # X_test = normalize(X_test, axis=1) # ì´ ì¤„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # search ê²°ê³¼ëŠ” D (ê±°ë¦¬)ì™€ I (ì¸ë±ìŠ¤)ì…ë‹ˆë‹¤.\n",
    "    # L2 ê±°ë¦¬ì´ë¯€ë¡œ ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ê°€ê¹Œìš´ ê²ƒì…ë‹ˆë‹¤.\n",
    "    distances, indices = index.search(X_test, n_neighbors) # <-- ì´ë¦„ ë³€ê²½ (similarities -> distances)\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        neighbor_labels = y_train_labels[indices[i]]\n",
    "        # ê±°ë¦¬ ê¸°ë°˜ì´ë¯€ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒë“¤ì˜ ë ˆì´ë¸”ì„ í†µí•´ ë‹¤ìˆ˜ê²° íˆ¬í‘œ\n",
    "        unique_labels, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "        predicted_label = unique_labels[np.argmax(counts)]\n",
    "        y_pred.append(predicted_label)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "def predict_faiss_knn_euclidean_topk(index, y_train_labels, n_neighbors, X_test):\n",
    "    \"\"\"\n",
    "    Faiss Top-K ì˜ˆì¸¡ ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì˜ˆ: k=10ì¼ ë•Œ ê° ìƒ˜í”Œë§ˆë‹¤ 10ê°œ ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸)\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    distances, indices = index.search(X_test, n_neighbors)\n",
    "\n",
    "    topk_labels = []\n",
    "    for i in range(len(X_test)):\n",
    "        neighbor_labels = y_train_labels[indices[i]]\n",
    "        topk_labels.append(neighbor_labels.tolist())\n",
    "    return topk_labels\n",
    "\n",
    "def task2_score(y_true, topk_preds, topk=10):\n",
    "    \"\"\"\n",
    "    ê° ìƒ˜í”Œë§ˆë‹¤ Top-10 ì˜ˆì¸¡ ì¤‘ ì •ë‹µì´ ëª‡ ë²ˆ í¬í•¨ë˜ì—ˆëŠ”ì§€ â†’ í‰ê·  ë¹„ìœ¨ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    match_counts = [\n",
    "        pred_list.count(true_label) / topk\n",
    "        for true_label, pred_list in zip(y_true, topk_preds)\n",
    "    ]\n",
    "    return np.mean(match_counts)\n",
    "\n",
    "# test() í•¨ìˆ˜\n",
    "def test(model_tuple, X_test, y_test, average='weighted', topk=10):\n",
    "    faiss_index, y_train_labels, n_neighbors = model_tuple\n",
    "    \n",
    "    print(f\"  â–¶ KNN ì˜ˆì¸¡ ì¤‘ (Faiss ì‚¬ìš©, k={n_neighbors})...\")\n",
    "    \n",
    "    # Top-1 ì˜ˆì¸¡ (ê¸°ì¡´)\n",
    "    y_pred = predict_faiss_knn_euclidean(faiss_index, y_train_labels, n_neighbors, X_test)\n",
    "\n",
    "    # Top-k ì˜ˆì¸¡\n",
    "    topk_labels = predict_faiss_knn_euclidean_topk(faiss_index, y_train_labels, topk, X_test)\n",
    "\n",
    "    # Top-10 Accuracy ê³„ì‚°\n",
    "    topk_hits = [true in pred_list for true, pred_list in zip(y_test, topk_labels)]\n",
    "    topk_acc = np.mean(topk_hits)\n",
    "\n",
    "    print(\"  âœ” ì˜ˆì¸¡ ì™„ë£Œ.\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    print(f\"[Top-1 Accuracy]  {acc:.4f}\")\n",
    "    print(f\"[Top-{topk} Accuracy] {topk_acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    print(f\"[Task2 Accuracy (Top-10 Same-Class)] {task2_acc:.4f}\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', normalize=False):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    \"\"\"ì´ë¯¸ì§€ í•˜ë‚˜ì— ëŒ€í•´ ì—¬ëŸ¬ ì¦ê°• ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ ì ìš©\"\"\"\n",
    "    choice = random.choice(['flip', 'rotate', 'brightness', 'blur', 'noise'])\n",
    "\n",
    "    if choice == 'flip':\n",
    "        return cv2.flip(img, 1)  # ì¢Œìš°ë°˜ì „\n",
    "    elif choice == 'rotate':\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "        return cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    elif choice == 'brightness':\n",
    "        factor = random.uniform(0.7, 1.3)\n",
    "        return np.clip(img.astype(np.float32) * factor, 0, 255).astype(np.uint8)\n",
    "    elif choice == 'blur':\n",
    "        return cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    elif choice == 'noise':\n",
    "        noise = np.random.normal(0, 10, img.shape).astype(np.uint8)\n",
    "        return cv2.add(img, noise)\n",
    "    return img  # fallback\n",
    "\n",
    "def augment_to_balance_min(df, target_min_per_class=500, seed=42):\n",
    "    \"\"\"í´ë˜ìŠ¤ë³„ë¡œ ìµœì†Œ target ê°œìˆ˜ë¥¼ í™•ë³´í•˜ë„ë¡ ì¦ê°• (ë§ì€ í´ë˜ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€)\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    augmented_rows = []\n",
    "\n",
    "    for label, group in df.groupby('label'):\n",
    "        n_current = len(group)\n",
    "        n_needed = target_min_per_class - n_current\n",
    "\n",
    "        if n_needed <= 0:\n",
    "            print(f\"âœ… {label}: {n_current}ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\")\n",
    "            augmented_rows.append(group)  # ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ë¼ë‚´ì§€ ì•ŠìŒ!)\n",
    "        else:\n",
    "            print(f\"â• {label}: {n_current}ê°œ â†’ {target_min_per_class}ê°œë¡œ ì¦ê°• ì¤‘ ({n_needed}ê°œ ìƒì„±)\")\n",
    "            samples_to_augment = group.sample(n=n_needed, replace=True, random_state=seed)\n",
    "\n",
    "            new_rows = []\n",
    "            for _, row in samples_to_augment.iterrows():\n",
    "                new_img = augment_image(row['image_data'])\n",
    "                new_rows.append({\n",
    "                    'label': row['label'],\n",
    "                    'image_data': new_img\n",
    "                })\n",
    "            augmented_rows.append(group)\n",
    "            augmented_rows.append(pd.DataFrame(new_rows))\n",
    "\n",
    "    balanced_df = pd.concat(augmented_rows).reset_index(drop=True)\n",
    "    return balanced_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í´ë” ë¡œë“œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:06<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ì´ 12700ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ.\n",
      "âœ” ë¼ë²¨ ì¸ì½”ë”© ì™„ë£Œ. í´ë˜ìŠ¤: ['Bicycle' 'Bridge' 'Bus' 'Car' 'Chimney' 'Crosswalk' 'Hydrant'\n",
      " 'Motorcycle' 'Palm' 'Traffic Light']\n",
      "âœ… Bicycle: 1299ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Bridge: 553ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Bus: 1500ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Car: 1500ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "â• Chimney: 431ê°œ â†’ 500ê°œë¡œ ì¦ê°• ì¤‘ (69ê°œ ìƒì„±)\n",
      "âœ… Crosswalk: 1260ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Hydrant: 1032ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Motorcycle: 681ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Palm: 932ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n",
      "âœ… Traffic Light: 905ê°œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì¦ê°• ì—†ìŒ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\bvb09\\AppData\\Local\\Temp\\ipykernel_8440\\2476747336.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  images_df = images_df.groupby('label').apply(\n"
     ]
    }
   ],
   "source": [
    "DATASET_BASE_PATH = \"C:/Users/bvb09/recaptcha-dataset\" \n",
    "\n",
    "try:\n",
    "    images_df = load_images_from_folder(DATASET_BASE_PATH)\n",
    "    print(f\"âœ” ì´ {len(images_df)}ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "\n",
    "# 2. ë¼ë²¨ ì¸ì½”ë”©\n",
    "le = LabelEncoder()\n",
    "images_df['label_encoded'] = le.fit_transform(images_df['label'])\n",
    "print(f\"âœ” ë¼ë²¨ ì¸ì½”ë”© ì™„ë£Œ. í´ë˜ìŠ¤: {le.classes_}\")\n",
    "\n",
    "images_df = images_df.groupby('label').apply(\n",
    "    lambda g: g.sample(n=1500, random_state=42) if g.name in ['Car', 'Bus'] else g\n",
    ").reset_index(drop=True)\n",
    "\n",
    "images_df = augment_to_balance_min(images_df)\n",
    "images_df['label_encoded'] = le.fit_transform(images_df['label'])\n",
    "\n",
    "# ì „ì²´ ì´ë¯¸ì§€ ë°ì´í„°ì— ëŒ€í•´ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "#print(\"â³ ì „ì²´ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "#images_df['processed_image_data'] = [preprocess_image(img) for img in tqdm(images_df['image_data'], desc=\"ì´ë¯¸ì§€ ì „ì²˜ë¦¬\")]\n",
    "#print(\"âœ” ì „ì²´ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘ (Color, HOG, LBP, SIFT descriptors)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Color Histogram ì¶”ì¶œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10162/10162 [00:00<00:00, 17042.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ì´ë¯¸ì§€ ë°ì´í„°ì— ëŒ€í•´ ê¸°ë³¸ íŠ¹ì§• ë¯¸ë¦¬ ì¶”ì¶œ\n",
    "print(\"â³ ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘ (Color, HOG, LBP, SIFT descriptors)...\")\n",
    "features_color_all = np.array([extract_color_histogram_features(img) for img in tqdm(images_df['image_data'], desc=\"Color Histogram ì¶”ì¶œ\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LBP ì¶”ì¶œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10162/10162 [00:18<00:00, 551.49it/s]\n"
     ]
    }
   ],
   "source": [
    "features_lbp_all = np.array([extract_lbp_features_from_array(img) for img in tqdm(images_df['image_data'], desc=\"LBP ì¶”ì¶œ\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HOG ì¶”ì¶œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10162/10162 [00:14<00:00, 709.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10162, 1296)\n"
     ]
    }
   ],
   "source": [
    "hog_list = [\n",
    "    extract_hog_features(img)\n",
    "    for img in tqdm(images_df['image_data'], desc=\"HOG ì¶”ì¶œ\")\n",
    "]\n",
    "features_hog_all = np.vstack(hog_list)  # shape = (n_images, hog_dim)\n",
    "print(features_hog_all.shape)\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "features_hog_all = pca.fit_transform(features_hog_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_glcm_all = np.array([extract_glcm_features(img) for img in tqdm(images_df['image_data'], desc=\"GLCM ì¶”ì¶œ\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Laws' Texture ì¶”ì¶œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10162/10162 [01:00<00:00, 167.93it/s]\n"
     ]
    }
   ],
   "source": [
    "features_laws_all = np.array([extract_laws_energy_features(img) for img in tqdm(images_df['image_data'], desc=\"Laws' Texture ì¶”ì¶œ\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce_descriptors(sift_descriptors_list_all, n_components=64):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ì´ë¯¸ì§€ì˜ SIFT ë””ìŠ¤í¬ë¦½í„°ì— ëŒ€í•´ PCAë¥¼ ì ìš©í•´ ì°¨ì› ì¶•ì†Œ.\n",
    "    PCAëŠ” ì „ì²´ ë””ìŠ¤í¬ë¦½í„°ì—ì„œ 1íšŒ í•™ìŠµí•œ ë’¤, ê° ë””ìŠ¤í¬ë¦½í„°ì— transformë§Œ ì ìš©.\n",
    "    \"\"\"\n",
    "    # ìœ íš¨í•œ ë””ìŠ¤í¬ë¦½í„°ë§Œ ì¶”ì¶œ (None ì œê±°, ìµœì†Œ ì°¨ì› ì´ìƒ)\n",
    "    valid_descriptors = [d for d in sift_descriptors_list_all if d is not None and len(d) >= n_components]\n",
    "    \n",
    "    if len(valid_descriptors) == 0:\n",
    "        print(\"âŒ PCA í•™ìŠµí•  ë””ìŠ¤í¬ë¦½í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ. ì°¨ì› ì¶•ì†Œë¥¼ ìƒëµí•©ë‹ˆë‹¤.\")\n",
    "        return sift_descriptors_list_all\n",
    "\n",
    "    # ì „ì²´ ë””ìŠ¤í¬ë¦½í„° ê²°í•©\n",
    "    all_descriptors = np.vstack(valid_descriptors)\n",
    "\n",
    "    # PCA í•™ìŠµ\n",
    "    print(f\"ğŸ” PCA í•™ìŠµ ì¤‘... (ì…ë ¥ ì°¨ì›: {all_descriptors.shape[1]} â†’ {n_components})\")\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    pca.fit(all_descriptors)\n",
    "\n",
    "    # ë³‘ë ¬ ë³€í™˜ í•¨ìˆ˜\n",
    "    def transform_if_valid(desc):\n",
    "        if desc is None or len(desc) < n_components:\n",
    "            return desc\n",
    "        return pca.transform(desc)\n",
    "\n",
    "    # ë³‘ë ¬ ì ìš©\n",
    "    print(\"âš™ PCA ë³€í™˜ ë³‘ë ¬ ì ìš© ì¤‘...\")\n",
    "    reduced_descriptors = Parallel(n_jobs=6)(\n",
    "        delayed(transform_if_valid)(desc) for desc in sift_descriptors_list_all\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… PCA ì¶•ì†Œ ì™„ë£Œ\")\n",
    "    return reduced_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIFT Descriptors ì¶”ì¶œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10162/10162 [01:39<00:00, 102.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: 10162\n",
      "ğŸ“Š PCA í•™ìŠµì— ì‚¬ìš©ëœ ë””ìŠ¤í¬ë¦½í„°ê°€ ìˆëŠ” ì´ë¯¸ì§€ ìˆ˜: 10162\n",
      "ğŸ“Š PCA í•™ìŠµì— ì‚¬ìš©ëœ ì „ì²´ SIFT ë²¡í„° ìˆ˜: 1756451\n"
     ]
    }
   ],
   "source": [
    "sift_descriptors_list_all = [extract_sift_descriptors_from_array(img) for img in tqdm(images_df['image_data'], desc=\"SIFT Descriptors ì¶”ì¶œ\")]\n",
    "\n",
    "# ì „ì²´ ì´ë¯¸ì§€ ìˆ˜\n",
    "total_count = len(sift_descriptors_list_all)\n",
    "\n",
    "# PCA í•™ìŠµì— ì‚¬ìš©í•  ë””ìŠ¤í¬ë¦½í„°ë§Œ í•„í„°ë§ (128ì°¨ì›ì¸ ê²½ìš°ë§Œ)\n",
    "all_descriptors = [des for des in sift_descriptors_list_all if des is not None and des.shape[1] == 128]\n",
    "pca_image_count = len(all_descriptors)  # ë””ìŠ¤í¬ë¦½í„°ê°€ ì¡´ì¬í•˜ê³  128ì°¨ì›ì¸ ì´ë¯¸ì§€ ìˆ˜\n",
    "\n",
    "X_all = np.vstack(all_descriptors)\n",
    "\n",
    "# PCA í•™ìŠµ\n",
    "pca = PCA(n_components=64, random_state=42)\n",
    "pca.fit(X_all)\n",
    "\n",
    "print(f\"ğŸ“Š ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {total_count}\")\n",
    "print(f\"ğŸ“Š PCA í•™ìŠµì— ì‚¬ìš©ëœ ë””ìŠ¤í¬ë¦½í„°ê°€ ìˆëŠ” ì´ë¯¸ì§€ ìˆ˜: {pca_image_count}\")\n",
    "print(f\"ğŸ“Š PCA í•™ìŠµì— ì‚¬ìš©ëœ ì „ì²´ SIFT ë²¡í„° ìˆ˜: {X_all.shape[0]}\")\n",
    "\n",
    "# PCA ë³€í™˜ ì ìš©\n",
    "sift_descriptors_list_all = [\n",
    "    pca.transform(des) if des is not None and des.shape[1] == 128 else None\n",
    "    for des in sift_descriptors_list_all\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©í•  íŠ¹ì§• ì¡°í•© ì •ì˜\n",
    "#feature_names = ['color', 'hog', 'lbp', 'sift']\n",
    "all_feature_combinations_tuples = []\n",
    "#from itertools import combinations\n",
    "#for i in range(1, len(feature_names) + 1):\n",
    "#    for combo in combinations(feature_names, i):\n",
    "#        all_feature_combinations_tuples.append(combo)\n",
    "\n",
    "# ì‚¬ìš©ìê°€ ìš”ì²­í•œ íŠ¹ì • ì¡°í•© ì¶”ê°€ (í•„ìš”ì‹œ)\n",
    "user_requested_combinations = [\n",
    " ('color', 'lbp'), ('color', 'lbp', 'hog'),\n",
    " ('color', 'law', 'hog'),\n",
    " ('color', 'lbp', 'sift', 'hog')\n",
    "]\n",
    "all_feature_combinations_tuples.extend(user_requested_combinations)\n",
    "# # ì¤‘ë³µ ì œê±°\n",
    "# all_feature_combinations_tuples = sorted(list(set(all_feature_combinations_tuples)))\n",
    "\n",
    "\n",
    "print(f\"\\nâ–¶ ì´ {len(all_feature_combinations_tuples)}ê°œì˜ íŠ¹ì§• ì¡°í•©ì— ëŒ€í•´ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "for combo in all_feature_combinations_tuples:\n",
    "    print(f\"  - {combo}\")\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "results = {}\n",
    "confusion_matrices = {}\n",
    "\n",
    "X_indices = np.arange(len(images_df)) # StratifiedKFoldì— ì‚¬ìš©í•  ì¸ë±ìŠ¤\n",
    "y_labels = images_df['label_encoded'].values\n",
    "\n",
    "# Stratified K-Fold ì¤€ë¹„\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # ë…¸íŠ¸ë¶ì˜ ê¸°ë³¸ 5-fold ì‚¬ìš©\n",
    "print(\"\\nâœ” Stratified K-Fold ì„¤ì • ì™„ë£Œ (5-Fold)\")\n",
    "\n",
    "# êµì°¨ ê²€ì¦ ë£¨í”„\n",
    "num_bovw_clusters = 200 # BoVW í´ëŸ¬ìŠ¤í„° ìˆ˜ (ë…¸íŠ¸ë¶ì˜ get_features_for_split í•¨ìˆ˜ ë‚´ num_bovw_clusters=200 ì°¸ê³ , ì¤„ì—¬ì„œ í…ŒìŠ¤íŠ¸)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_indices, y_labels)):\n",
    "    print(f\"\\n=============== FOLD {fold+1}/5 ================\")\n",
    "    y_train_fold, y_val_fold = y_labels[train_idx], y_labels[val_idx]\n",
    "    # í˜„ì¬ í´ë“œì˜ SIFT ë””ìŠ¤í¬ë¦½í„° (í›ˆë ¨ ë°ì´í„°ìš©, ê²€ì¦ ë°ì´í„°ìš©)\n",
    "    sift_descriptors_train_fold = [sift_descriptors_list_all[i] for i in train_idx]\n",
    "    sift_descriptors_val_fold = [sift_descriptors_list_all[i] for i in val_idx]\n",
    "\n",
    "    print(f\"  â³ BoVW Vocabulary í•™ìŠµ ì¤‘ (Fold {fold+1})\")\n",
    "    # SIFT ë””ìŠ¤í¬ë¦½í„°ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°ë¥¼ ë°©ì§€\n",
    "    valid_sift_descriptors_train_fold = [d for d in sift_descriptors_train_fold if d is not None and len(d) > 0]\n",
    "    bovw_vocabulary = learn_bovw_vocabulary(valid_sift_descriptors_train_fold, num_clusters=num_bovw_clusters)\n",
    "\n",
    "\n",
    "    # ê° íŠ¹ì§• ì¡°í•©ì— ëŒ€í•´ í•™ìŠµ ë° í‰ê°€\n",
    "    for feature_combo_tuple in all_feature_combinations_tuples:\n",
    "        feature_combo_name = '+'.join(feature_combo_tuple)\n",
    "        print(f\"\\n--- ì¡°í•©: {feature_combo_name} (Fold {fold+1}) ---\")\n",
    "        \n",
    "        X_train_features_list = []\n",
    "        X_val_features_list = []\n",
    "        \n",
    "        if 'color' in feature_combo_tuple:\n",
    "            X_train_features_list.append(features_color_all[train_idx])\n",
    "            X_val_features_list.append(features_color_all[val_idx])\n",
    "        if 'hog' in feature_combo_tuple:\n",
    "            X_train_features_list.append(features_hog_all[train_idx])\n",
    "            X_val_features_list.append(features_hog_all[val_idx])\n",
    "        if 'lbp' in feature_combo_tuple:\n",
    "            X_train_features_list.append(features_lbp_all[train_idx])\n",
    "            X_val_features_list.append(features_lbp_all[val_idx])\n",
    "        if 'laws' in feature_combo_tuple:\n",
    "            X_train_features_list.append(features_laws_all[train_idx])\n",
    "            X_val_features_list.append(features_laws_all[val_idx])    \n",
    "        if 'sift' in feature_combo_tuple:\n",
    "            if not valid_sift_descriptors_train_fold:\n",
    "                print(f\"  âš ï¸ ê²½ê³ : Fold {fold+1}ì˜ í›ˆë ¨ ë°ì´í„°ì— ìœ íš¨í•œ SIFT ë””ìŠ¤í¬ë¦½í„°ê°€ ì—†ì–´ BoVW íŠ¹ì§•ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì¡°í•©/í´ë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                # ë¹ˆ íˆìŠ¤í† ê·¸ë¨ ë˜ëŠ” ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "                num_features_sift = num_bovw_clusters \n",
    "                train_bovw_hist = np.zeros((len(train_idx), num_features_sift))\n",
    "                val_bovw_hist = np.zeros((len(val_idx), num_features_sift))\n",
    "            else:\n",
    "                if bovw_vocabulary is None or bovw_vocabulary.shape[0] == 0 :\n",
    "                     print(f\"  âš ï¸ ê²½ê³ : Fold {fold+1}ì—ì„œ BoVW vocabulary í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´ ì¡°í•©/í´ë“œëŠ” ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                     num_features_sift = num_bovw_clusters \n",
    "                     train_bovw_hist = np.zeros((len(train_idx), num_features_sift))\n",
    "                     val_bovw_hist = np.zeros((len(val_idx), num_features_sift))\n",
    "                else:\n",
    "                    print(f\"  âœ” BoVW Vocabulary í•™ìŠµ ì™„ë£Œ ({bovw_vocabulary.shape[0]} clusters).\")\n",
    "                    print(f\"  â³ BoVW Histogram ìƒì„± ì¤‘ (Fold {fold+1}, ì¡°í•© {feature_combo_name})...\")\n",
    "                    # ë³‘ë ¬ ì²˜ë¦¬ + ë²¡í„°í™”ëœ íˆìŠ¤í† ê·¸ë¨ ìƒì„±\n",
    "                    train_bovw_hist = np.array(parallel_create_bovw_histograms(sift_descriptors_train_fold, bovw_vocabulary, n_jobs=-1))\n",
    "                    val_bovw_hist = np.array(parallel_create_bovw_histograms(sift_descriptors_val_fold, bovw_vocabulary, n_jobs=-1))\n",
    "\n",
    "            X_train_features_list.append(train_bovw_hist)\n",
    "            X_val_features_list.append(val_bovw_hist)\n",
    "\n",
    "        if not X_train_features_list: # íŠ¹ì§•ì´ í•˜ë‚˜ë„ ì„ íƒë˜ì§€ ì•Šì€ ê²½ìš° (ì¼ì–´ë‚  ì¼ ì—†ì§€ë§Œ ë°©ì–´ ì½”ë“œ)\n",
    "            print(f\"  âš ï¸ ê²½ê³ : íŠ¹ì§•ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ ({feature_combo_name}). ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "\n",
    "        # íŠ¹ì§• ê²°í•©\n",
    "        X_train_combined = combine_features(*X_train_features_list)\n",
    "        X_val_combined = combine_features(*X_val_features_list)\n",
    "        \n",
    "        # ë°ì´í„° ì •ê·œí™” (í•„ìš”ì‹œ ì¶”ê°€ - ì˜ˆ: StandardScaler ë˜ëŠ” MinMaxScaler)\n",
    "        # scaler = StandardScaler()\n",
    "        # X_train_combined = scaler.fit_transform(X_train_combined)\n",
    "        # X_val_combined = scaler.transform(X_val_combined)\n",
    "        # print(\"  â„¹ï¸ íŠ¹ì§• ì •ê·œí™” ì ìš©ë¨.\")\n",
    "\n",
    "        print(f\"  â³ Faiss KNN ëª¨ë¸ í•™ìŠµ ì¤‘ (k=3, Fold {fold+1}, ì¡°í•© {feature_combo_name})...\")\n",
    "        # NaN ë˜ëŠ” Inf ê°’ í™•ì¸ ë° ì²˜ë¦¬ (ì¤‘ìš”!)\n",
    "        if np.isnan(X_train_combined).any() or np.isinf(X_train_combined).any():\n",
    "            print(f\"  âš ï¸ ê²½ê³ : X_train_combinedì— NaN ë˜ëŠ” Inf ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. (ì¡°í•©: {feature_combo_name}, Fold: {fold+1})\")\n",
    "            X_train_combined = np.nan_to_num(X_train_combined, nan=0.0, posinf=0.0, neginf=0.0) # ë˜ëŠ” ë‹¤ë¥¸ ëŒ€ì²´ ì „ëµ\n",
    "        if np.isnan(X_val_combined).any() or np.isinf(X_val_combined).any():\n",
    "            print(f\"  âš ï¸ ê²½ê³ : X_val_combinedì— NaN ë˜ëŠ” Inf ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. (ì¡°í•©: {feature_combo_name}, Fold: {fold+1})\")\n",
    "            X_val_combined = np.nan_to_num(X_val_combined, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # íŠ¹ì§• ë²¡í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬ (L2 ì •ê·œí™” ë“±ì—ì„œ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)\n",
    "        if X_train_combined.shape[1] == 0:\n",
    "            print(f\"  âš ï¸ ê²½ê³ : í›ˆë ¨ íŠ¹ì§• ë²¡í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ì¡°í•©: {feature_combo_name}, Fold: {fold+1}). ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "        \n",
    "        # FaissëŠ” float32 íƒ€ì…ì„ ìš”êµ¬í•¨\n",
    "        X_train_combined = X_train_combined.astype(np.float32)\n",
    "        X_val_combined = X_val_combined.astype(np.float32)\n",
    "\n",
    "        try:\n",
    "            # train KNN\n",
    "            faiss_index, train_labels_for_pred, n_neighbors_actual = train_faiss_knn_euclidean(\n",
    "                X_train_combined, y_train_fold, n_neighbors=10\n",
    "            )\n",
    "    \n",
    "            print(f\"  â–¶ KNN ì˜ˆì¸¡ ì¤‘ (Faiss ì‚¬ìš©, k={n_neighbors_actual}, Fold {fold+1}, ì¡°í•© {feature_combo_name})...\")\n",
    "\n",
    "            # Top-k label list\n",
    "            y_pred_topk = predict_faiss_knn_euclidean_topk(\n",
    "                faiss_index, train_labels_for_pred, n_neighbors_actual, X_val_combined\n",
    "            )\n",
    "\n",
    "            # Top-1 predictions (ê°€ì¥ ì•ì— ìˆëŠ” ê²ƒë§Œ ì‚¬ìš©)\n",
    "            y_pred_fold = [row[0] for row in y_pred_topk]\n",
    "\n",
    "            # ê¸°ì¡´ í‰ê°€\n",
    "            acc = accuracy_score(y_val_fold, y_pred_fold)\n",
    "            fsc = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "\n",
    "            # â¬‡ï¸ Top-10 Accuracy ê³„ì‚°\n",
    "            top10_hits = [true in top10 for true, top10 in zip(y_val_fold, y_pred_topk)]\n",
    "            top10_acc = np.mean(top10_hits)\n",
    "\n",
    "            def task2_score(y_true, topk_preds, topk=10):\n",
    "                return np.mean([preds.count(t) / topk for t, preds in zip(y_true, topk_preds)])\n",
    "            task2 = task2_score(y_val_fold, y_pred_topk, topk=10)\n",
    "\n",
    "            print(f\"  âœ” ì˜ˆì¸¡ ì™„ë£Œ.\")\n",
    "            print(f\"    â”œ Top-1 Accuracy : {acc:.4f}\")\n",
    "            print(f\"    â”œ Top-10 Accuracy: {top10_acc:.4f} (ì •ë‹µì´ Top-10 ì•ˆì— ìˆìœ¼ë©´ ì¸ì •)\")\n",
    "            print(f\"    â”” Task2 Score    : {task2:.4f} (Top-10 ì¤‘ ì •ë‹µ ë“±ì¥ ë¹„ìœ¨ í‰ê· )\")\n",
    "            print(f\"  âœ” F1-Score: {fsc:.4f} (Fold {fold+1}, ì¡°í•© {feature_combo_name})\")\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_val_fold, y_pred_fold)\n",
    "            if feature_combo_name not in confusion_matrices:\n",
    "                confusion_matrices[feature_combo_name] = {}\n",
    "            confusion_matrices[feature_combo_name][fold] = cm\n",
    "\n",
    "            if feature_combo_name not in results:\n",
    "                results[feature_combo_name] = []\n",
    "            results[feature_combo_name].append(acc)\n",
    "\n",
    "            # ì‹œê°í™”\n",
    "            plot_confusion_matrix(\n",
    "                cm,\n",
    "                class_names=['Bicycle','Bridge','Bus', 'Car', 'Chimney','Crosswalk','Hydrant','Motorcycle','Palm','Traffic Light'],\n",
    "                title=f'{feature_combo_name} - Fold {fold+1}',\n",
    "                normalize=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ ì˜¤ë¥˜ ë°œìƒ (Fold {fold+1}, ì¡°í•© {feature_combo_name}): {e}\")\n",
    "            if feature_combo_name not in results:\n",
    "                results[feature_combo_name] = []\n",
    "            results[feature_combo_name].append(np.nan)\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ì§‘ê³„ ë° ì¶œë ¥\n",
    "print(\"\\n\\n=============== ìµœì¢… êµì°¨ ê²€ì¦ ê²°ê³¼ ================\")\n",
    "for combo_name, acc_list in results.items():\n",
    "    valid_acc_list = [acc for acc in acc_list if not np.isnan(acc)]\n",
    "    if valid_acc_list:\n",
    "        mean_acc = np.mean(valid_acc_list)\n",
    "        std_acc = np.std(valid_acc_list)\n",
    "        print(f\"íŠ¹ì§• ì¡°í•©: {combo_name}\")\n",
    "        for i, acc_fold in enumerate(acc_list):\n",
    "             print(f\"  Fold {i+1} Accuracy: {acc_fold:.4f}\" if not np.isnan(acc_fold) else f\"  Fold {i+1} Accuracy: Error\")\n",
    "        print(f\"  >> í‰ê·  ì •í™•ë„: {mean_acc:.4f} (í‘œì¤€í¸ì°¨: {std_acc:.4f})\")\n",
    "    else:\n",
    "        print(f\"íŠ¹ì§• ì¡°í•©: {combo_name} - ëª¨ë“  í´ë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ ë˜ëŠ” ìœ íš¨í•œ ê²°ê³¼ ì—†ìŒ.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì¡°í•© ì°¾ê¸° (í‰ê·  ì •í™•ë„ ê¸°ì¤€)\n",
    "if results:\n",
    "    sorted_results = sorted(results.items(), key=lambda item: np.nanmean(item[1]) if item[1] else -1, reverse=True)\n",
    "    print(\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ì¡°í•© (í‰ê·  ì •í™•ë„ ê¸°ì¤€):\")\n",
    "    if sorted_results and np.nanmean(sorted_results[0][1]):\n",
    "         best_combo_name, best_acc_list = sorted_results[0]\n",
    "         print(f\"  {best_combo_name}: í‰ê·  ì •í™•ë„ = {np.nanmean(best_acc_list):.4f}\")\n",
    "    else:\n",
    "        print(\"  ìœ íš¨í•œ ê²°ê³¼ë¥¼ ê°€ì§„ ì¡°í•©ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
