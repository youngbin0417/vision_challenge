{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30afab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bvb09\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm # ë¡œì»¬ì—ì„œëŠ” tqdm.notebook ëŒ€ì‹  ì¼ë°˜ tqdm ì‚¬ìš©\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "# skimage\n",
    "from skimage.exposure import rescale_intensity, equalize_hist\n",
    "from skimage.filters import gaussian\n",
    "from skimage.restoration import denoise_bilateral\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops\n",
    "from skimage.transform import resize\n",
    "from scipy import ndimage\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, confusion_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# joblib (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# faiss (KNN ê°€ì†í™”ë¥¼ ìœ„í•´)\n",
    "import faiss\n",
    "\n",
    "# ë°ì´í„° ê´€ë ¨ \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a24971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def edge(img_bgr):\n",
    "    # 1) ê°•ì œ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    img_bgr = cv2.resize(img_bgr, (120, 120), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 2) CLAHE â†’ Gray â†’ Blur â†’ Denoise â†’ Canny\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(2.0, (8,8))\n",
    "    l = clahe.apply(l)\n",
    "    img_eq = cv2.cvtColor(cv2.merge((l,a,b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    gray = cv2.cvtColor(img_eq, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    denoised = cv2.fastNlMeansDenoising(blurred, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "    edges = cv2.Canny(denoised, 100, 230)   \n",
    "\n",
    "    return edges\n",
    "\n",
    "# 1) ê³µí†µ: BGR â†’ LAB â†’ CLAHE(L) â†’ BGR (í•„ìš” ì‹œ)\n",
    "def apply_clahe(img_bgr):\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l = cv2.createCLAHE(2.0, (8,8)).apply(l)\n",
    "    return cv2.cvtColor(cv2.merge((l,a,b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# 3) Unsharp mask (SIFT ì „ìš©)\n",
    "def unsharp(img_gray):\n",
    "    blurred = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "    return cv2.addWeighted(img_gray, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "# 4) Mild Gaussian blur (LBP/GLCM/Laws)\n",
    "def mild_blur(img_gray):\n",
    "    return cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "def gray(img):\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return image\n",
    "\n",
    "# ==============================================================================\n",
    "#  í”¼ì³ ì¶”ì¶œ í•¨ìˆ˜\n",
    "# ==================================================================:============\n",
    "\n",
    "def extract_color_histogram_features(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0,1,2], None, (8,8,8), [0,180,0,256,0,256])\n",
    "    return cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "def extract_sift_pca_mean(img_bgr, pca_model=None, n_components=32):\n",
    "    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_gray, None)\n",
    "\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        return np.zeros(n_components if pca_model else 128, dtype=np.float32)\n",
    "\n",
    "    if pca_model is None:\n",
    "        pca_model = PCA(n_components=n_components)\n",
    "        descriptors_pca = pca_model.fit_transform(descriptors)\n",
    "    else:\n",
    "        descriptors_pca = pca_model.transform(descriptors)\n",
    "\n",
    "    mean_vector = np.mean(descriptors_pca, axis=0)\n",
    "    return mean_vector.astype(np.float32)\n",
    "\n",
    "def extract_glcm_features(image):\n",
    "    if image is None:\n",
    "        num_props = 6\n",
    "        num_distances = 3\n",
    "        num_angles = 4\n",
    "        return np.zeros(num_props * num_distances * num_angles)\n",
    "        \n",
    "    img_glcm = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_glcm = mild_blur(img_glcm)\n",
    "    img_glcm = apply_clahe(cv2.cvtColor(img_glcm, cv2.COLOR_GRAY2BGR))\n",
    "    gray_image = cv2.cvtColor(img_glcm, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    distances = [1, 2, 3]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    \n",
    "    try:\n",
    "        glcm = graycomatrix(gray_image, distances=distances, angles=angles, symmetric=True, normed=True)\n",
    "        \n",
    "        props_to_extract = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "        glcm_features = []\n",
    "        for prop in props_to_extract:\n",
    "            glcm_features.append(graycoprops(glcm, prop).ravel())\n",
    "            \n",
    "        return np.concatenate(glcm_features)\n",
    "    except Exception as e:\n",
    "        print(f\"GLCM ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        num_props = 6\n",
    "        num_distances = len(distances) \n",
    "        num_angles = len(angles)   \n",
    "        return np.zeros(num_props * num_distances * num_angles)\n",
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2)):\n",
    "    img = cv2.resize(image, (120, 120), interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    features = hog(gray,\n",
    "                   orientations=orientations,\n",
    "                   pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block,\n",
    "                   block_norm='L2-Hys',\n",
    "                   visualize=False,\n",
    "                   transform_sqrt=True,\n",
    "                   feature_vector=True)\n",
    "    return features.astype(np.float32)\n",
    "\n",
    "def extract_sift_descriptors_from_array(image):\n",
    "    img = apply_clahe(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image=unsharp(img)\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    return des\n",
    "\n",
    "def extract_lbp_features_from_array(image, P=8, R=1, method='uniform'):\n",
    "    image=apply_clahe(image)\n",
    "    image=mild_blur(image)\n",
    "    gray_image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp_image = local_binary_pattern(gray_image, P, R, method=method)\n",
    "\n",
    "    max_bins = P * (P - 1) + 3 if method == 'default' else P + 2\n",
    "    hist, _ = np.histogram(lbp_image.ravel(), bins=max_bins, range=(0, max_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "# Laws' Texture Energy - ê¸°ì¡´ê³¼ ë™ì¼\n",
    "def extract_laws_energy_features(image, window_size=15):\n",
    "    image_gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_gray=mild_blur(image_gray)\n",
    "\n",
    "    L5 = np.array([1, 4, 6, 4, 1], dtype=np.float32)\n",
    "    E5 = np.array([-1, -2, 0, 2, 1], dtype=np.float32)\n",
    "    S5 = np.array([-1, 0, 2, 0, -1], dtype=np.float32)\n",
    "    W5 = np.array([-1, 2, 0, -2, 1], dtype=np.float32)\n",
    "    R5 = np.array([1, -4, 6, -4, 1], dtype=np.float32)\n",
    "    kernels = [L5, E5, S5, W5, R5]\n",
    "\n",
    "    energy_features = []\n",
    "    if image_gray.dtype == np.uint8:\n",
    "        image_gray = image_gray.astype(np.float32)\n",
    "\n",
    "    for k1 in kernels:\n",
    "        for k2 in kernels:\n",
    "            kernel = np.outer(k1, k2)\n",
    "            filtered = ndimage.convolve(image_gray, kernel, mode='reflect')\n",
    "            energy = np.abs(filtered)\n",
    "            summed = cv2.boxFilter(energy, ddepth=-1, ksize=(window_size, window_size), normalize=False)\n",
    "            energy_features.append(summed.mean())\n",
    "\n",
    "    return np.array(energy_features, dtype=np.float32)\n",
    "\n",
    "\n",
    "def learn_bovw_vocabulary(all_sift_descriptors, num_clusters=200):\n",
    "    filtered = [des for des in all_sift_descriptors if des is not None and len(des) > 0]\n",
    "    feature_dims = {des.shape[1] for des in filtered}\n",
    "    if len(feature_dims) > 1:\n",
    "        raise ValueError(f\"âŒ BoVW í•™ìŠµìš© SIFT ë””ìŠ¤í¬ë¦½í„°ë“¤ì˜ ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ: {feature_dims}\")\n",
    "\n",
    "    concatenated_descriptors = np.vstack(filtered)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    _, _, centers = cv2.kmeans(\n",
    "        concatenated_descriptors.astype(np.float32),\n",
    "        num_clusters, None, criteria, 10, flags\n",
    "    )\n",
    "    return centers\n",
    "\n",
    "def create_bovw_histogram(sift_descriptors, vocabulary):\n",
    "    num_clusters = vocabulary.shape[0]\n",
    "    if sift_descriptors is None or len(sift_descriptors) == 0:\n",
    "        return np.zeros(num_clusters, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        distances = np.linalg.norm(\n",
    "            vocabulary[None, :, :] - sift_descriptors[:, None, :], axis=2\n",
    "        )\n",
    "        closest_clusters = np.argmin(distances, axis=1)\n",
    "        histogram = np.bincount(closest_clusters, minlength=num_clusters).astype(np.float32)\n",
    "        histogram = cv2.normalize(histogram, None, norm_type=cv2.NORM_L2).flatten()\n",
    "        return histogram\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ BoVW íˆìŠ¤í† ê·¸ë¨ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "        return np.zeros(num_clusters, dtype=np.float32)\n",
    "\n",
    "\n",
    "def parallel_create_bovw_histograms(descriptor_list, vocabulary, n_jobs=6):\n",
    "    histograms = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(create_bovw_histogram)(desc, vocabulary) for desc in descriptor_list\n",
    "    )\n",
    "    return np.array(histograms, dtype=np.float32)\n",
    "\n",
    "print(\"âœ” íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  ë¶„ë¥˜ ëª¨ë¸ ë° í•™ìŠµ/í‰ê°€ í•¨ìˆ˜\n",
    "# ==============================================================================\n",
    "# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ Faiss KNN í•™ìŠµ (ìˆ˜ì •ë¨)\n",
    "\n",
    "def combine_features(*feature_arrays):\n",
    "    \"\"\"ì£¼ì–´ì§„ íŠ¹ì§• ë°°ì—´ë“¤ì„ ê°€ë¡œë¡œ ê²°í•©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return np.hstack(feature_arrays)\n",
    "\n",
    "def train_faiss_knn_euclidean(X_train, y_train, n_neighbors=3):\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    D = X_train.shape[1]\n",
    "\n",
    "    index = faiss.IndexFlatL2(D)\n",
    "    index.add(X_train)\n",
    "\n",
    "    return index, y_train, n_neighbors\n",
    "\n",
    "# ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜ Faiss KNN ì˜ˆì¸¡ (ìˆ˜ì •ë¨)\n",
    "def predict_faiss_knn_euclidean(index, y_train_labels, n_neighbors, X_test):\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    distances, indices = index.search(X_test, n_neighbors)\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        neighbor_labels = y_train_labels[indices[i]]\n",
    "        unique_labels, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "        predicted_label = unique_labels[np.argmax(counts)]\n",
    "        y_pred.append(predicted_label)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# ê¸°ì¡´ predict_faiss_knn_euclidean í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì•„ë˜ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "# ì´ í•¨ìˆ˜ëŠ” kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì˜ ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "def predict_faiss_knn_topk(faiss_index, train_labels_encoded, query_features, k=10):\n",
    "    if faiss_index is None:\n",
    "        raise ValueError(\"Faiss ì¸ë±ìŠ¤ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    if query_features.shape[0] == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    D, I = faiss_index.search(query_features, k)\n",
    "    predicted_neighbor_labels = train_labels_encoded[I]\n",
    "\n",
    "    return D, I, predicted_neighbor_labels\n",
    "\n",
    "# ì°¸ê³ : ê¸°ì¡´ predict_faiss_knn_euclidean í•¨ìˆ˜ëŠ” ë‹¨ìˆœíˆ k=1ë¡œ ì„¤ì •í•˜ê³  ì²« ë²ˆì§¸ ë¼ë²¨ì„ ë°˜í™˜í•˜ëŠ” ì‹ìœ¼ë¡œ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "# ë§Œì•½ ê¸°ì¡´ predict_faiss_knn_euclidean í•¨ìˆ˜ê°€ kë¥¼ ì¸ìë¡œ ë°›ëŠ”ë‹¤ë©´, ê·¸ í•¨ìˆ˜ë¥¼ í™œìš©í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "# ì—¬ê¸°ì„œëŠ” Top-k ê²°ê³¼ë¥¼ ì§ì ‘ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ predict_faiss_knn_topkë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "# Task2 Accuracy (Top-K Same-Class)ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "def task2_score(y_true, topk_preds, topk=10):\n",
    "    match_counts = []\n",
    "    for true_label, pred_list_np in zip(y_true, topk_preds):\n",
    "        # NumPy ë°°ì—´ì„ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ .count() ë©”ì„œë“œ ì‚¬ìš©\n",
    "        pred_list = pred_list_np.tolist()\n",
    "        # topk_predsì˜ ê° ìš”ì†Œ(pred_list)ëŠ” ì‹¤ì œ ë¼ë²¨ì´ í¬í•¨ëœ íšŸìˆ˜ / topk\n",
    "        # ì˜ˆë¥¼ ë“¤ì–´, topk=10ì¼ ë•Œ, 'car' ë¼ë²¨ì´ 3ë²ˆ ë“±ì¥í•˜ë©´ 3/10 = 0.3\n",
    "        match_counts.append(pred_list.count(true_label) / topk)\n",
    "    return np.mean(match_counts)\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', normalize=False):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b431ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ ê²½ë¡œ ì„¤ì •\n",
    "MODEL_DIR = './model_outputs2'\n",
    "QUERY_IMAGE_DIR = 'C:/Users/bvb09/Downloads/query/query'  # <- ì—¬ê¸°ì— ì¿¼ë¦¬ ì´ë¯¸ì§€ í´ë” ë„£ê¸°\n",
    "\n",
    "# ğŸ”¹ ëª¨ë¸ ë° ë„êµ¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "faiss_index = faiss.read_index(os.path.join(MODEL_DIR, 'faiss_index.idx'))\n",
    "le = joblib.load(os.path.join(MODEL_DIR, 'label_encoder.pkl'))\n",
    "pca_hog = joblib.load(os.path.join(MODEL_DIR, 'pca_hog.pkl'))\n",
    "pca_sift = joblib.load(os.path.join(MODEL_DIR, 'pca_sift.pkl'))\n",
    "bovw_vocabulary = np.load(os.path.join(MODEL_DIR, 'bovw_vocabulary.npy'))\n",
    "train_labels_for_pred = joblib.load(os.path.join(MODEL_DIR, 'labels.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eade782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. ì¿¼ë¦¬ ì´ë¯¸ì§€ ë¡œë“œ (ë¼ë²¨ ì—†ìŒ)\n",
    "# ================================\n",
    "def load_images_nolabel(folder_path, resize_to=(120, 120)):\n",
    "    \"\"\"\n",
    "    í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  (ì˜µì…˜) ë¦¬ì‚¬ì´ì¦ˆí•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    images, image_paths = [], []\n",
    "    valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
    "\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(valid_exts):\n",
    "            img_path = os.path.join(folder_path, fname)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                if resize_to is not None:\n",
    "                    img = cv2.resize(img, resize_to, interpolation=cv2.INTER_AREA)\n",
    "                images.append(img)\n",
    "                image_paths.append(img_path)\n",
    "            else:\n",
    "                print(f\"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}\")\n",
    "\n",
    "    return images, image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8006eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” ì¿¼ë¦¬ ì´ë¯¸ì§€ 100ì¥ ë¡œë“œ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ì´ë¯¸ì§€ ë¡œë”©\n",
    "images, image_paths = load_images_nolabel(QUERY_IMAGE_DIR)\n",
    "print(f\"âœ” ì¿¼ë¦¬ ì´ë¯¸ì§€ {len(images)}ì¥ ë¡œë“œ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d256e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Color: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 14291.62it/s]\n",
      "LAWS: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 176.42it/s]\n",
      "HOG: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 835.83it/s]\n",
      "SIFT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 321.93it/s]\n"
     ]
    }
   ],
   "source": [
    "features_color = np.array([extract_color_histogram_features(img) for img in tqdm(images, desc=\"Color\")])\n",
    "features_laws = np.array([extract_laws_energy_features(img) for img in tqdm(images, desc=\"LAWS\")])\n",
    "hog_list = [extract_hog_features(img) for img in tqdm(images, desc=\"HOG\")]\n",
    "features_hog = pca_hog.transform(np.vstack(hog_list))\n",
    "\n",
    "sift_des_list = [extract_sift_descriptors_from_array(img) for img in tqdm(images, desc=\"SIFT\")]\n",
    "sift_des_list = [\n",
    "    pca_sift.transform(d) if d is not None and d.shape[1] == 128 else None\n",
    "    for d in sift_des_list\n",
    "]\n",
    "\n",
    "valid_des = [d for d in sift_des_list if d is not None]\n",
    "bovw_hist_partial = parallel_create_bovw_histograms(valid_des, bovw_vocabulary, n_jobs=-1)\n",
    "\n",
    "bovw_hist = []\n",
    "idx = 0\n",
    "for d in sift_des_list:\n",
    "    if d is not None:\n",
    "        bovw_hist.append(bovw_hist_partial[idx])\n",
    "        idx += 1\n",
    "    else:\n",
    "        bovw_hist.append(np.zeros(bovw_vocabulary.shape[0]))\n",
    "bovw_hist = np.array(bovw_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac6aaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ” Top-10 ì˜ˆì¸¡ ì™„ë£Œ: (100, 10)\n",
      "âœ… Top-1 ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: ./model_outputs2\\c1_t1_a2.csv\n",
      "âœ… Top-10 ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: ./model_outputs2\\c1_t2_a2.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4. íŠ¹ì§• ë²¡í„° ê²°í•© ë° ì˜ˆì¸¡\n",
    "# ================================\n",
    "X_combined = combine_features(features_color, features_laws, features_hog, bovw_hist).astype(np.float32)\n",
    "\n",
    "# ğŸ”¹ Faissë¡œ Top-K ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "topk = 10\n",
    "D, I, predicted_neighbor_labels_topk = predict_faiss_knn_topk(\n",
    "    faiss_index, train_labels_for_pred, X_combined, k=topk\n",
    ")\n",
    "\n",
    "# ğŸ”¹ Top-1 ì˜ˆì¸¡ ê²°ê³¼ (ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì˜ ë¼ë²¨)\n",
    "predicted_labels_encoded_top1 = predicted_neighbor_labels_topk[:, 0]\n",
    "\n",
    "print(f\"\\nâœ” Top-{topk} ì˜ˆì¸¡ ì™„ë£Œ: {predicted_neighbor_labels_topk.shape}\")\n",
    "\n",
    "# ================================\n",
    "# 5. í‰ê°€ ì§€í‘œ ì¶œë ¥\n",
    "# ================================\n",
    "# ğŸ”¹ Top-1 ì˜ˆì¸¡ ë¼ë²¨ (ë¬¸ìì—´)\n",
    "predicted_labels_top1 = le.inverse_transform(predicted_labels_encoded_top1)\n",
    "\n",
    "# ğŸ”¹ ì´ë¯¸ì§€ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ\n",
    "image_names = [os.path.basename(path) for path in image_paths]\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ DataFrame\n",
    "results_simple_df = pd.DataFrame({\n",
    "    'image': image_names,\n",
    "    'predicted_label': predicted_labels_top1\n",
    "})\n",
    "\n",
    "# ğŸ”¹ ì €ì¥\n",
    "csv_path = os.path.join(MODEL_DIR, 'c1_t1_a2.csv')\n",
    "results_simple_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… Top-1 ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {csv_path}\")\n",
    "\n",
    "# ğŸ”¹ í™•ì¸\n",
    "results_simple_df.head()\n",
    "\n",
    "# ğŸ”¹ ë¼ë²¨ ë¬¸ìì—´ ë³€í™˜\n",
    "topk_labels_str = [le.inverse_transform(row) for row in predicted_neighbor_labels_topk]\n",
    "\n",
    "# ğŸ”¹ íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ\n",
    "image_names = [os.path.basename(p) for p in image_paths]\n",
    "\n",
    "# ğŸ”¹ DataFrame ìƒì„±\n",
    "topk_df = pd.DataFrame(topk_labels_str)\n",
    "topk_df.insert(0, 'image', image_names)  # ì²« ì—´ì— ì´ë¯¸ì§€ ì´ë¦„ ì¶”ê°€\n",
    "\n",
    "# ğŸ”¹ ì €ì¥\n",
    "csv_path = os.path.join(MODEL_DIR, 'c1_t2_a2.csv')\n",
    "topk_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… Top-10 ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
