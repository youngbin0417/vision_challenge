{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bvb09\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 모든 라이브러리 임포트 완료!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  라이브러리 임포트\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm # 로컬에서는 tqdm.notebook 대신 일반 tqdm 사용\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "# skimage\n",
    "from skimage.exposure import rescale_intensity, equalize_hist\n",
    "from skimage.filters import gaussian\n",
    "from skimage.restoration import denoise_bilateral\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.feature import local_binary_pattern, hog, graycomatrix, graycoprops\n",
    "from skimage.transform import resize\n",
    "from scipy import ndimage\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, confusion_matrix\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# joblib (병렬 처리를 위해)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# faiss (KNN 가속화를 위해)\n",
    "import faiss\n",
    "\n",
    "# 데이터 관련 \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#DATASET_BASE_PATH = \"C:/Users/bvb09/.cache/kagglehub/datasets/mikhailma/test-dataset/versions/1/Google_Recaptcha_V2_Images_Dataset\"\n",
    "\n",
    "print(\"✔ 모든 라이브러리 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  데이터 로드 함수\n",
    "# ==============================================================================\n",
    "\n",
    "# 이미지 로드 함수\n",
    "def load_images_from_folder(base_path):\n",
    "    \"\"\"\n",
    "    주어진 base_path 내의 서브폴더(라벨)에서 이미지를 로드하고,\n",
    "    이미지 데이터, 라벨, 그리고 원본 이미지 경로를 포함하는 DataFrame을 반환합니다.\n",
    "    (예: base_path/label_name/image.jpg 구조를 가정)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = [] # 이미지 경로를 저장할 리스트 추가\n",
    "\n",
    "    if not os.path.exists(base_path):\n",
    "        raise FileNotFoundError(f\"지정된 폴더를 찾을 수 없습니다: {base_path}\\n\"\n",
    "                                f\"경로를 올바르게 설정했는지 확인해주세요.\")\n",
    "\n",
    "    print(f\"⏳ Loading images from: {base_path}\")\n",
    "    # base_path 바로 아래의 모든 폴더(라벨)를 순회합니다.\n",
    "    for label_name in tqdm(os.listdir(base_path), desc=\"폴더 로드 중\"):\n",
    "        label_path = os.path.join(base_path, label_name)\n",
    "        \n",
    "        # 이것이 실제 라벨 폴더인지 확인합니다.\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_name in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_name)\n",
    "                # 이미지 파일만 처리하도록 확장자 필터링\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    try:\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            images.append(img)\n",
    "                            labels.append(label_name)\n",
    "                            image_paths.append(img_path) # 원본 이미지 경로 저장\n",
    "                        else:\n",
    "                            print(f\"경고: {img_path} 이미지를 로드할 수 없습니다. (로드 실패)\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"경고: {img_path} 로딩 중 오류 발생 - {e}\")\n",
    "                # else: (이미지 확장자가 아닌 파일은 무시)\n",
    "        # else: (base_path 바로 아래에 파일이 있는 경우는 무시, 라벨 폴더 구조를 가정)\n",
    "\n",
    "    df = pd.DataFrame({'image_data': images, 'label': labels, 'image_path': image_paths})\n",
    "    print(f\"✔ 총 {len(df)}개의 이미지 로드 완료.\")\n",
    "    return df\n",
    "\n",
    "def visualize_features(X_feats, y_labels, method='pca'):\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    else:\n",
    "        from sklearn.manifold import TSNE\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "    X_reduced = reducer.fit_transform(X_feats)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_labels, cmap='tab20', s=10, alpha=0.7)\n",
    "    plt.title(f'Feature Distribution via {method.upper()}')\n",
    "    plt.colorbar(scatter, ticks=range(len(set(y_labels))))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge(img_bgr):\n",
    "    # 1) 강제 리사이즈\n",
    "    img_bgr = cv2.resize(img_bgr, (120, 120), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 2) CLAHE → Gray → Blur → Denoise → Canny\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(2.0, (8,8))\n",
    "    l = clahe.apply(l)\n",
    "    img_eq = cv2.cvtColor(cv2.merge((l,a,b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    gray = cv2.cvtColor(img_eq, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    denoised = cv2.fastNlMeansDenoising(blurred, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "    edges = cv2.Canny(denoised, 100, 230)   \n",
    "\n",
    "    return edges\n",
    "\n",
    "# 1) 공통: BGR → LAB → CLAHE(L) → BGR (필요 시)\n",
    "def apply_clahe(img_bgr):\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    l = cv2.createCLAHE(2.0, (8,8)).apply(l)\n",
    "    return cv2.cvtColor(cv2.merge((l,a,b)), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "# 3) Unsharp mask (SIFT 전용)\n",
    "def unsharp(img_gray):\n",
    "    blurred = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "    return cv2.addWeighted(img_gray, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "# 4) Mild Gaussian blur (LBP/GLCM/Laws)\n",
    "def mild_blur(img_gray):\n",
    "    return cv2.GaussianBlur(img_gray, (3,3), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(img):\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkcol(img_bgr):\n",
    "    #image = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    #image=cv2.GaussianBlur(image, ksize=(3,3), sigmaX=0)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    #image = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, (8,8,8), [0,180, 0,256, 0,256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    hist_rgb = cv2.calcHist([rgb], [0, 1, 2], None, (8,8,8), [0,256, 0,256, 0,256])\n",
    "    hist_rgb = cv2.normalize(hist_rgb, hist_rgb).flatten()\n",
    "    hist_bgr = cv2.calcHist([img_bgr], [0, 1, 2], None, (8,8,8), [0,256, 0,256, 0,256])\n",
    "    hist_bgr = cv2.normalize(hist_bgr, hist_bgr).flatten()\n",
    "    #plt.imshow(v)\n",
    "    #plt.title(\"Color\")\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "     # ✅ 2D 시각화\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(hist, color='blue')\n",
    "    plt.title(\"Flattened HSV Histogram\")\n",
    "    plt.xlabel(\"Bin Index\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return hsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 특징 추출 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  피쳐 추출 함수\n",
    "# ==================================================================:============\n",
    "\n",
    "def extract_color_histogram_features(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0,1,2], None, (8,8,8), [0,180,0,256,0,256])\n",
    "    return cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "def extract_sift_pca_mean(img_bgr, pca_model=None, n_components=32):\n",
    "    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_gray, None)\n",
    "\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        return np.zeros(n_components if pca_model else 128, dtype=np.float32)\n",
    "\n",
    "    if pca_model is None:\n",
    "        pca_model = PCA(n_components=n_components)\n",
    "        descriptors_pca = pca_model.fit_transform(descriptors)\n",
    "    else:\n",
    "        descriptors_pca = pca_model.transform(descriptors)\n",
    "\n",
    "    mean_vector = np.mean(descriptors_pca, axis=0)\n",
    "    return mean_vector.astype(np.float32)\n",
    "\n",
    "def extract_glcm_features(image):\n",
    "    if image is None:\n",
    "        num_props = 6\n",
    "        num_distances = 3\n",
    "        num_angles = 4\n",
    "        return np.zeros(num_props * num_distances * num_angles)\n",
    "        \n",
    "    img_glcm = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_glcm = mild_blur(img_glcm)\n",
    "    img_glcm = apply_clahe(cv2.cvtColor(img_glcm, cv2.COLOR_GRAY2BGR))\n",
    "    gray_image = cv2.cvtColor(img_glcm, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    distances = [1, 2, 3]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    \n",
    "    try:\n",
    "        glcm = graycomatrix(gray_image, distances=distances, angles=angles, symmetric=True, normed=True)\n",
    "        \n",
    "        props_to_extract = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "        glcm_features = []\n",
    "        for prop in props_to_extract:\n",
    "            glcm_features.append(graycoprops(glcm, prop).ravel())\n",
    "            \n",
    "        return np.concatenate(glcm_features)\n",
    "    except Exception as e:\n",
    "        print(f\"GLCM 추출 중 오류 발생: {e}\")\n",
    "        num_props = 6\n",
    "        num_distances = len(distances) \n",
    "        num_angles = len(angles)   \n",
    "        return np.zeros(num_props * num_distances * num_angles)\n",
    "\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2)):\n",
    "    img = cv2.resize(image, (120, 120), interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    features = hog(gray,\n",
    "                   orientations=orientations,\n",
    "                   pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block,\n",
    "                   block_norm='L2-Hys',\n",
    "                   visualize=False,\n",
    "                   transform_sqrt=True,\n",
    "                   feature_vector=True)\n",
    "    return features.astype(np.float32)\n",
    "\n",
    "def extract_sift_descriptors_from_array(image):\n",
    "    img = apply_clahe(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image=unsharp(img)\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "    return des\n",
    "\n",
    "def extract_lbp_features_from_array(image, P=8, R=1, method='uniform'):\n",
    "    image=apply_clahe(image)\n",
    "    image=mild_blur(image)\n",
    "    gray_image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp_image = local_binary_pattern(gray_image, P, R, method=method)\n",
    "\n",
    "    max_bins = P * (P - 1) + 3 if method == 'default' else P + 2\n",
    "    hist, _ = np.histogram(lbp_image.ravel(), bins=max_bins, range=(0, max_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "# Laws' Texture Energy - 기존과 동일\n",
    "def extract_laws_energy_features(image, window_size=15):\n",
    "    image_gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_gray=mild_blur(image_gray)\n",
    "\n",
    "    L5 = np.array([1, 4, 6, 4, 1], dtype=np.float32)\n",
    "    E5 = np.array([-1, -2, 0, 2, 1], dtype=np.float32)\n",
    "    S5 = np.array([-1, 0, 2, 0, -1], dtype=np.float32)\n",
    "    W5 = np.array([-1, 2, 0, -2, 1], dtype=np.float32)\n",
    "    R5 = np.array([1, -4, 6, -4, 1], dtype=np.float32)\n",
    "    kernels = [L5, E5, S5, W5, R5]\n",
    "\n",
    "    energy_features = []\n",
    "    if image_gray.dtype == np.uint8:\n",
    "        image_gray = image_gray.astype(np.float32)\n",
    "\n",
    "    for k1 in kernels:\n",
    "        for k2 in kernels:\n",
    "            kernel = np.outer(k1, k2)\n",
    "            filtered = ndimage.convolve(image_gray, kernel, mode='reflect')\n",
    "            energy = np.abs(filtered)\n",
    "            summed = cv2.boxFilter(energy, ddepth=-1, ksize=(window_size, window_size), normalize=False)\n",
    "            energy_features.append(summed.mean())\n",
    "\n",
    "    return np.array(energy_features, dtype=np.float32)\n",
    "\n",
    "\n",
    "def learn_bovw_vocabulary(all_sift_descriptors, num_clusters=200):\n",
    "    filtered = [des for des in all_sift_descriptors if des is not None and len(des) > 0]\n",
    "    feature_dims = {des.shape[1] for des in filtered}\n",
    "    if len(feature_dims) > 1:\n",
    "        raise ValueError(f\"❌ BoVW 학습용 SIFT 디스크립터들의 차원이 일치하지 않음: {feature_dims}\")\n",
    "\n",
    "    concatenated_descriptors = np.vstack(filtered)\n",
    "    \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    _, _, centers = cv2.kmeans(\n",
    "        concatenated_descriptors.astype(np.float32),\n",
    "        num_clusters, None, criteria, 10, flags\n",
    "    )\n",
    "    return centers\n",
    "\n",
    "def create_bovw_histogram(sift_descriptors, vocabulary):\n",
    "    num_clusters = vocabulary.shape[0]\n",
    "    if sift_descriptors is None or len(sift_descriptors) == 0:\n",
    "        return np.zeros(num_clusters, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        distances = np.linalg.norm(\n",
    "            vocabulary[None, :, :] - sift_descriptors[:, None, :], axis=2\n",
    "        )\n",
    "        closest_clusters = np.argmin(distances, axis=1)\n",
    "        histogram = np.bincount(closest_clusters, minlength=num_clusters).astype(np.float32)\n",
    "        histogram = cv2.normalize(histogram, None, norm_type=cv2.NORM_L2).flatten()\n",
    "        return histogram\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ BoVW 히스토그램 생성 오류: {e}\")\n",
    "        return np.zeros(num_clusters, dtype=np.float32)\n",
    "\n",
    "\n",
    "def parallel_create_bovw_histograms(descriptor_list, vocabulary, n_jobs=6):\n",
    "    histograms = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(create_bovw_histogram)(desc, vocabulary) for desc in descriptor_list\n",
    "    )\n",
    "    return np.array(histograms, dtype=np.float32)\n",
    "\n",
    "print(\"✔ 특징 추출 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  분류 모델 및 학습/평가 함수\n",
    "# ==============================================================================\n",
    "# 유클리드 거리 기반 Faiss KNN 학습 (수정됨)\n",
    "\n",
    "def combine_features(*feature_arrays):\n",
    "    \"\"\"주어진 특징 배열들을 가로로 결합합니다.\"\"\"\n",
    "    return np.hstack(feature_arrays)\n",
    "\n",
    "def train_faiss_knn_euclidean(X_train, y_train, n_neighbors=3):\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    D = X_train.shape[1]\n",
    "\n",
    "    index = faiss.IndexFlatL2(D)\n",
    "    index.add(X_train)\n",
    "\n",
    "    return index, y_train, n_neighbors\n",
    "\n",
    "# 유클리드 거리 기반 Faiss KNN 예측 (수정됨)\n",
    "def predict_faiss_knn_euclidean(index, y_train_labels, n_neighbors, X_test):\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    distances, indices = index.search(X_test, n_neighbors)\n",
    "\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        neighbor_labels = y_train_labels[indices[i]]\n",
    "        unique_labels, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "        predicted_label = unique_labels[np.argmax(counts)]\n",
    "        y_pred.append(predicted_label)\n",
    "    return np.array(y_pred)\n",
    "\n",
    "# 기존 predict_faiss_knn_euclidean 함수를 수정하거나, 아래 함수를 추가합니다.\n",
    "# 이 함수는 k개의 가장 가까운 이웃의 라벨을 반환합니다.\n",
    "def predict_faiss_knn_topk(faiss_index, train_labels_encoded, query_features, k=10):\n",
    "    if faiss_index is None:\n",
    "        raise ValueError(\"Faiss 인덱스가 학습되지 않았습니다.\")\n",
    "    if query_features.shape[0] == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    D, I = faiss_index.search(query_features, k)\n",
    "    predicted_neighbor_labels = train_labels_encoded[I]\n",
    "\n",
    "    return D, I, predicted_neighbor_labels\n",
    "\n",
    "# 참고: 기존 predict_faiss_knn_euclidean 함수는 단순히 k=1로 설정하고 첫 번째 라벨을 반환하는 식으로 동작할 수 있습니다.\n",
    "# 만약 기존 predict_faiss_knn_euclidean 함수가 k를 인자로 받는다면, 그 함수를 활용해도 좋습니다.\n",
    "# 여기서는 Top-k 결과를 직접 다룰 수 있도록 predict_faiss_knn_topk를 사용합니다.\n",
    "\n",
    "# Task2 Accuracy (Top-K Same-Class)를 계산하는 함수\n",
    "def task2_score(y_true, topk_preds, topk=10):\n",
    "    match_counts = []\n",
    "    for true_label, pred_list_np in zip(y_true, topk_preds):\n",
    "        # NumPy 배열을 Python 리스트로 변환하여 .count() 메서드 사용\n",
    "        pred_list = pred_list_np.tolist()\n",
    "        # topk_preds의 각 요소(pred_list)는 실제 라벨이 포함된 횟수 / topk\n",
    "        # 예를 들어, topk=10일 때, 'car' 라벨이 3번 등장하면 3/10 = 0.3\n",
    "        match_counts.append(pred_list.count(true_label) / topk)\n",
    "    return np.mean(match_counts)\n",
    "\n",
    "def test(model_tuple, X_test, y_test, average='weighted', topk=10):\n",
    "    faiss_index, y_train_labels, n_neighbors = model_tuple\n",
    "    \n",
    "    print(f\"  ▶ KNN 예측 중 (Faiss 사용, k={n_neighbors})...\")\n",
    "    \n",
    "    # Top-1 예측 (기존)\n",
    "    y_pred = predict_faiss_knn_euclidean(faiss_index, y_train_labels, n_neighbors, X_test)\n",
    "\n",
    "    # Top-k 예측을 위해 predict_faiss_knn_topk 사용\n",
    "    # D, I는 필요 없으므로 _ 처리\n",
    "    _, _, topk_predicted_labels_encoded = predict_faiss_knn_topk(faiss_index, y_train_labels, X_test, k=topk)\n",
    "\n",
    "    print(\"  ✔ 예측 완료.\")\n",
    "    \n",
    "    # Top-1 Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Top-k Accuracy 계산 (기존 방식)\n",
    "    topk_correct_count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        # y_test[i] (실제 라벨)이 상위 k개 예측 라벨 중 하나라도 있는지 확인\n",
    "        if y_test[i] in topk_predicted_labels_encoded[i]:\n",
    "            topk_correct_count += 1\n",
    "    topk_acc = topk_correct_count / len(y_test)\n",
    "\n",
    "    # --- 여기에 Task2 Accuracy (Top-10 Same-Class) 계산 및 출력 추가 ---\n",
    "    # task2_score 함수가 정의되어 있어야 합니다.\n",
    "    # 만약 task2_score 함수가 아직 정의되지 않았다면, 이전에 제공된 코드를 추가해야 합니다.\n",
    "    # (예: def task2_score(y_true, topk_preds, topk=10): ... )\n",
    "    task2_acc = task2_score(y_test, topk_predicted_labels_encoded, topk=topk)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    print(f\"[Top-1 Accuracy]    {acc:.4f}\")\n",
    "    print(f\"[Task2 Accuracy (Top-{topk} Same-Class)] {task2_acc:.4f}\") # 이 줄을 추가\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', normalize=False):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    choice = random.choice(['flip', 'rotate', 'brightness', 'blur', 'noise'])\n",
    "    if choice == 'flip':\n",
    "        return cv2.flip(img, 1)\n",
    "    elif choice == 'rotate':\n",
    "        angle = random.uniform(-15, 15)\n",
    "        h, w = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "        return cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "    elif choice == 'brightness':\n",
    "        factor = random.uniform(0.7, 1.3)\n",
    "        return np.clip(img.astype(np.float32) * factor, 0, 255).astype(np.uint8)\n",
    "    elif choice == 'blur':\n",
    "        return cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    elif choice == 'noise':\n",
    "        noise = np.random.normal(0, 10, img.shape).astype(np.uint8)\n",
    "        return cv2.add(img, noise)\n",
    "    return img\n",
    "\n",
    "def augment_to_balance_min(df, target_min_per_class=500, seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    augmented_rows = []\n",
    "\n",
    "    for label, group in df.groupby('label'):\n",
    "        n_current = len(group)\n",
    "        n_needed = target_min_per_class - n_current\n",
    "\n",
    "        if n_needed <= 0:\n",
    "            print(f\"✅ {label}: {n_current}개 → 그대로 유지 (증강 없음)\")\n",
    "            augmented_rows.append(group)\n",
    "        else:\n",
    "            print(f\"➕ {label}: {n_current}개 → {target_min_per_class}개로 증강 중 ({n_needed}개 생성)\")\n",
    "            samples_to_augment = group.sample(n=n_needed, replace=True, random_state=seed)\n",
    "\n",
    "            new_rows = []\n",
    "            for _, row in samples_to_augment.iterrows():\n",
    "                new_img = augment_image(row['image_data'])\n",
    "                # 증강된 이미지에는 image_path를 새로 할당할 필요가 없으므로 image_data만 추가\n",
    "                new_rows.append({\n",
    "                    'label': row['label'],\n",
    "                    'image_data': new_img\n",
    "                })\n",
    "            augmented_rows.append(group)\n",
    "            augmented_rows.append(pd.DataFrame(new_rows))\n",
    "\n",
    "    balanced_df = pd.concat(augmented_rows).reset_index(drop=True)\n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading images from: C:/Users/bvb09/recaptcha-dataset\\images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "폴더 로드 중: 100%|██████████| 10/10 [00:38<00:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 총 13408개의 이미지 로드 완료.\n",
      "✔ 총 13408개의 이미지 로드 완료.\n",
      "✔ 라벨 인코딩 완료. 클래스: ['Bicycle' 'Bridge' 'Bus' 'Car' 'Chimney' 'Crosswalk' 'Hydrant'\n",
      " 'Motorcycle' 'Palm' 'Traffic Light']\n",
      "✅ Bicycle: 1299개 → 그대로 유지 (증강 없음)\n",
      "✅ Bridge: 1278개 → 그대로 유지 (증강 없음)\n",
      "✅ Bus: 1500개 → 그대로 유지 (증강 없음)\n",
      "✅ Car: 1500개 → 그대로 유지 (증강 없음)\n",
      "➕ Chimney: 431개 → 500개로 증강 중 (69개 생성)\n",
      "✅ Crosswalk: 1260개 → 그대로 유지 (증강 없음)\n",
      "✅ Hydrant: 1032개 → 그대로 유지 (증강 없음)\n",
      "✅ Motorcycle: 679개 → 그대로 유지 (증강 없음)\n",
      "✅ Palm: 932개 → 그대로 유지 (증강 없음)\n",
      "✅ Traffic Light: 905개 → 그대로 유지 (증강 없음)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\bvb09\\AppData\\Local\\Temp\\ipykernel_4040\\2780136523.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  images_df = images_df.groupby('label').apply(\n"
     ]
    }
   ],
   "source": [
    "DATASET_BASE_PATH = \"C:/Users/bvb09/recaptcha-dataset\" \n",
    "\n",
    "try:\n",
    "    images_df = load_images_from_folder(os.path.join(DATASET_BASE_PATH, 'images'))\n",
    "    print(f\"✔ 총 {len(images_df)}개의 이미지 로드 완료.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: {e}\")\n",
    "    \n",
    "\n",
    "# 2. 라벨 인코딩\n",
    "le = LabelEncoder()\n",
    "images_df['label_encoded'] = le.fit_transform(images_df['label'])\n",
    "print(f\"✔ 라벨 인코딩 완료. 클래스: {le.classes_}\")\n",
    "\n",
    "images_df = images_df.groupby('label').apply(\n",
    "    lambda g: g.sample(n=1500, random_state=42) if g.name in ['Car', 'Bus'] else g\n",
    ").reset_index(drop=True)\n",
    "\n",
    "images_df = augment_to_balance_min(images_df)\n",
    "images_df['label_encoded'] = le.fit_transform(images_df['label'])\n",
    "\n",
    "# 전체 이미지 데이터에 대해 전처리 수행\n",
    "#print(\"⏳ 전체 이미지 전처리 중...\")\n",
    "#images_df['processed_image_data'] = [preprocess_image(img) for img in tqdm(images_df['image_data'], desc=\"이미지 전처리\")]\n",
    "#print(\"✔ 전체 이미지 전처리 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Color Histogram 추출: 100%|██████████| 10885/10885 [00:00<00:00, 19104.68it/s]\n"
     ]
    }
   ],
   "source": [
    "features_color_all = np.array([extract_color_histogram_features(img) for img in tqdm(images_df['image_data'], desc=\"Color Histogram 추출\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LBP 추출: 100%|██████████| 10885/10885 [00:19<00:00, 563.99it/s]\n"
     ]
    }
   ],
   "source": [
    "features_lbp_all = np.array([extract_lbp_features_from_array(img) for img in tqdm(images_df['image_data'], desc=\"LBP 추출\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HOG 추출: 100%|██████████| 10885/10885 [00:14<00:00, 727.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - HOG 원본 특징 차원: (10885, 1296)\n",
      "  - HOG PCA 변환 완료. 변환 후 차원: (10885, 50)\n"
     ]
    }
   ],
   "source": [
    "hog_list = [\n",
    "    extract_hog_features(img)\n",
    "    for img in tqdm(images_df['image_data'], desc=\"HOG 추출\")\n",
    "]\n",
    "features_hog_all = np.vstack(hog_list)\n",
    "print(f\"  - HOG 원본 특징 차원: {features_hog_all.shape}\")\n",
    "pca_hog = PCA(n_components=50, random_state=42)\n",
    "features_hog_all = pca_hog.fit_transform(features_hog_all)\n",
    "print(f\"  - HOG PCA 변환 완료. 변환 후 차원: {features_hog_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_glcm_all = np.array([extract_glcm_features(img) for img in tqdm(images_df['image_data'], desc=\"GLCM 추출\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_laws_all = np.array([extract_laws_energy_features(img) for img in tqdm(images_df['image_data'], desc=\"Laws' Texture 추출\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce_descriptors(sift_descriptors_list_all, n_components=64):\n",
    "    \"\"\"\n",
    "    모든 이미지의 SIFT 디스크립터에 대해 PCA를 적용해 차원 축소.\n",
    "    PCA는 전체 디스크립터에서 1회 학습한 뒤, 각 디스크립터에 transform만 적용.\n",
    "    \"\"\"\n",
    "    # 유효한 디스크립터만 추출 (None 제거, 최소 차원 이상)\n",
    "    valid_descriptors = [d for d in sift_descriptors_list_all if d is not None and len(d) >= n_components]\n",
    "    \n",
    "    if len(valid_descriptors) == 0:\n",
    "        print(\"❌ PCA 학습할 디스크립터가 충분하지 않음. 차원 축소를 생략합니다.\")\n",
    "        return sift_descriptors_list_all\n",
    "\n",
    "    # 전체 디스크립터 결합\n",
    "    all_descriptors = np.vstack(valid_descriptors)\n",
    "\n",
    "    # PCA 학습\n",
    "    print(f\"🔍 PCA 학습 중... (입력 차원: {all_descriptors.shape[1]} → {n_components})\")\n",
    "    pca_sift = PCA(n_components=n_components, random_state=42)\n",
    "    pca_sift.fit(all_descriptors)\n",
    "\n",
    "    # 병렬 변환 함수\n",
    "    def transform_if_valid(desc):\n",
    "        if desc is None or len(desc) < n_components:\n",
    "            return desc\n",
    "        return pca_sift.transform(desc)\n",
    "\n",
    "    # 병렬 적용\n",
    "    print(\"⚙ PCA 변환 병렬 적용 중...\")\n",
    "    reduced_descriptors = Parallel(n_jobs=6)(\n",
    "        delayed(transform_if_valid)(desc) for desc in sift_descriptors_list_all\n",
    "    )\n",
    "    \n",
    "    print(\"✅ PCA 축소 완료\")\n",
    "    return reduced_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIFT Descriptors 추출: 100%|██████████| 10885/10885 [01:25<00:00, 127.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 전체 이미지 수: 10885\n",
      "📊 PCA 학습에 사용된 SIFT 디스크립터가 있는 이미지 수: 10885\n",
      "📊 PCA 학습에 사용된 전체 SIFT 벡터 수: 1862828\n",
      "✅ SIFT 디스크립터 PCA 변환 완료.\n"
     ]
    }
   ],
   "source": [
    "# SIFT 디스크립터 추출\n",
    "sift_descriptors_list_all = [extract_sift_descriptors_from_array(img) for img in tqdm(images_df['image_data'], desc=\"SIFT Descriptors 추출\")]\n",
    "\n",
    "# 전체 이미지 수\n",
    "total_count = len(sift_descriptors_list_all)\n",
    "\n",
    "# PCA 학습에 사용할 디스크립터만 필터링 (128차원인 경우만)\n",
    "all_descriptors = [des for des in sift_descriptors_list_all if des is not None and des.shape[1] == 128]\n",
    "pca_image_count = len(all_descriptors)\n",
    "\n",
    "if len(all_descriptors) > 0: # PCA 학습할 데이터가 있을 경우에만 진행\n",
    "    X_all_sift_for_pca = np.vstack(all_descriptors) # SIFT PCA를 위한 데이터\n",
    "    pca_sift = PCA(n_components=64, random_state=42)\n",
    "    pca_sift.fit(X_all_sift_for_pca)\n",
    "\n",
    "    print(f\"📊 전체 이미지 수: {total_count}\")\n",
    "    print(f\"📊 PCA 학습에 사용된 SIFT 디스크립터가 있는 이미지 수: {pca_image_count}\")\n",
    "    print(f\"📊 PCA 학습에 사용된 전체 SIFT 벡터 수: {X_all_sift_for_pca.shape[0]}\")\n",
    "\n",
    "    # PCA 변환 적용\n",
    "    sift_descriptors_list_all = [\n",
    "        pca_sift.transform(des) if des is not None and des.shape[1] == 128 else None\n",
    "        for des in sift_descriptors_list_all\n",
    "    ]\n",
    "    print(\"✅ SIFT 디스크립터 PCA 변환 완료.\")\n",
    "else:\n",
    "    print(\"⚠️ SIFT 디스크립터가 없거나 PCA 학습에 사용할 데이터가 충분하지 않습니다. SIFT PCA를 건너뜜니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 전체 BoVW Vocabulary 학습 완료 (200 clusters)\n",
      "  - BoVW Histogram 차원: (10885, 200)\n",
      "✔ 특징 추출 완료.\n",
      "✔ 학습 데이터 통합 특징 벡터 생성 완료. Shape: (10885, 772)\n",
      "✔ 전체 데이터로 Faiss KNN 학습 완료\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. 전체 SIFT → BoVW Vocabulary 학습\n",
    "# ----------------------------------------\n",
    "valid_sift_descriptors_all = [d for d in sift_descriptors_list_all if d is not None and len(d) > 0]\n",
    "if valid_sift_descriptors_all:\n",
    "    bovw_vocabulary = learn_bovw_vocabulary(valid_sift_descriptors_all, num_clusters=200)\n",
    "    print(f\"✔ 전체 BoVW Vocabulary 학습 완료 ({bovw_vocabulary.shape[0]} clusters)\")\n",
    "else:\n",
    "    print(\"⚠️ 유효한 SIFT 디스크립터가 없어 BoVW Vocabulary를 학습할 수 없습니다.\")\n",
    "    # Vocabulary가 없으면 BoVW 히스토그램 생성 시 오류가 발생하므로, 빈 Vocabulary를 정의합니다.\n",
    "    bovw_vocabulary = np.zeros((200, 64)) # 예시: 200 클러스터, SIFT PCA 차원 64로 가정\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. BoVW Histogram 전체 생성\n",
    "# ----------------------------------------\n",
    "bovw_hist_all = np.array(parallel_create_bovw_histograms(sift_descriptors_list_all, bovw_vocabulary, n_jobs=-1))\n",
    "print(f\"  - BoVW Histogram 차원: {bovw_hist_all.shape}\")\n",
    "print(\"✔ 특징 추출 완료.\")\n",
    "# ----------------------------------------\n",
    "# 3. 조합: color+lbp+sift+hog\n",
    "# ----------------------------------------\n",
    "feature_combo = ('color', 'lbp', 'sift', 'hog')\n",
    "X_features_list = [\n",
    "    features_color_all,\n",
    "    features_lbp_all,\n",
    "    features_hog_all,\n",
    "    bovw_hist_all\n",
    "]\n",
    "X_all_combined = combine_features(*X_features_list).astype(np.float32)\n",
    "y_all_encoded = images_df['label_encoded'].values\n",
    "print(f\"✔ 학습 데이터 통합 특징 벡터 생성 완료. Shape: {X_all_combined.shape}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Faiss KNN 전체 학습\n",
    "# ----------------------------------------\n",
    "faiss_index, train_labels_for_pred, _ = train_faiss_knn_euclidean(X_all_combined, y_all_encoded, n_neighbors=10)\n",
    "print(\"✔ 전체 데이터로 Faiss KNN 학습 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류 발생: 파일 저장에 실패했습니다. [Errno 2] No such file or directory: './model_outputs1/feautres.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  추출된 피처, 라벨, 학습된 모델들 저장 (새로 추가)\n",
    "# ==============================================================================\n",
    "\n",
    "SAVE_DIR = './model_outputs1'\n",
    "try:\n",
    "    joblib.dump(X_all_combined, './model_outputs1/feautres.pkl') # feature vector\n",
    "    joblib.dump(y_all_encoded, './model_outputs1/labels.pkl') # label\n",
    "    joblib.dump(le, './model_outputs1/label_encoder.pkl') # 라벨 인코더 저장\n",
    "    \n",
    "    joblib.dump(pca_hog, './model_outputs1/pca_hog.pkl')\n",
    "    joblib.dump(pca_sift, './model_outputs1/pca_sift.pkl')\n",
    "    \n",
    "    np.save(\"model_outputs1/bovw_vocabulary.npy\", bovw_vocabulary)\n",
    "    faiss.write_index(faiss_index, \"model_outputs1/faiss_index.idx\")\n",
    "\n",
    "    print(f\"✔ 모든 학습 관련 파일이 '{SAVE_DIR}' 폴더에 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: 파일 저장에 실패했습니다. {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading images from: C:/query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "폴더 로드 중: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ 총 0개의 이미지 로드 완료.\n",
      "✔ 총 0개의 쿼리 이미지 로드 완료.\n",
      "⏳ 쿼리 이미지 기본 특징 추출 중 (Color, HOG, LBP, SIFT descriptors)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query Color Histogram 추출: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Color Histogram 특징 차원: (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query LBP 추출: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - LBP 특징 차원: (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query HOG 추출: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# HOG 추출\u001b[39;00m\n\u001b[32m     21\u001b[39m query_hog_list = [\n\u001b[32m     22\u001b[39m     extract_hog_features(img)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(query_df[\u001b[33m'\u001b[39m\u001b[33mimage_data\u001b[39m\u001b[33m'\u001b[39m], desc=\u001b[33m\"\u001b[39m\u001b[33mQuery HOG 추출\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m query_features_hog = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_hog_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - HOG 원본 특징 차원: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_features_hog.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# HOG PCA 변환 (학습 시 사용한 pca_hog 모델을 그대로 사용)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bvb09\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\shape_base.py:292\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    291\u001b[39m     arrs = (arrs,)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  쿼리 이미지 처리 및 예측 (수정된 코드)\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. 쿼리 이미지 로드\n",
    "query_df = load_images_from_folder(\"C:/query/images\")\n",
    "print(f\"✔ 총 {len(query_df)}개의 쿼리 이미지 로드 완료.\")\n",
    "\n",
    "# 2. 쿼리 이미지에 대한 특징 추출\n",
    "print(\"⏳ 쿼리 이미지 기본 특징 추출 중 (Color, HOG, LBP, SIFT descriptors)...\")\n",
    "\n",
    "# Color Histogram 추출\n",
    "query_features_color = np.array([extract_color_histogram_features(img) for img in tqdm(query_df['image_data'], desc=\"Query Color Histogram 추출\")])\n",
    "print(f\"  - Color Histogram 특징 차원: {query_features_color.shape}\")\n",
    "\n",
    "# LBP 추출\n",
    "query_features_lbp = np.array([extract_lbp_features_from_array(img) for img in tqdm(query_df['image_data'], desc=\"Query LBP 추출\")])\n",
    "print(f\"  - LBP 특징 차원: {query_features_lbp.shape}\")\n",
    "\n",
    "# HOG 추출\n",
    "query_hog_list = [\n",
    "    extract_hog_features(img)\n",
    "    for img in tqdm(query_df['image_data'], desc=\"Query HOG 추출\")\n",
    "]\n",
    "query_features_hog = np.vstack(query_hog_list)\n",
    "print(f\"  - HOG 원본 특징 차원: {query_features_hog.shape}\")\n",
    "\n",
    "# HOG PCA 변환 (학습 시 사용한 pca_hog 모델을 그대로 사용)\n",
    "if 'pca_hog' in locals() and pca_hog is not None:\n",
    "    query_features_hog = pca_hog.transform(query_features_hog)\n",
    "    print(f\"  - HOG PCA 변환 완료. 변환 후 차원: {query_features_hog.shape}\")\n",
    "else:\n",
    "    print(\"  ⚠️ pca_hog 모델이 정의되지 않았습니다. HOG PCA를 건너뜁니다.\")\n",
    "\n",
    "# SIFT 디스크립터 추출\n",
    "query_sift_descriptors_list = [extract_sift_descriptors_from_array(img) for img in tqdm(query_df['image_data'], desc=\"Query SIFT Descriptors 추출\")]\n",
    "\n",
    "# SIFT 디스크립터 PCA 변환 (학습 시 사용한 pca_sift 모델을 그대로 사용)\n",
    "if 'pca_sift' in locals() and pca_sift is not None:\n",
    "    query_sift_descriptors_list = [\n",
    "        pca_sift.transform(des) if des is not None and des.shape[1] == 128 else None\n",
    "        for des in query_sift_descriptors_list\n",
    "    ]\n",
    "    print(f\"  - SIFT 디스크립터 PCA 변환 완료.\")\n",
    "else:\n",
    "    print(\"  ⚠️ pca_sift 모델이 정의되지 않았습니다. SIFT PCA를 건너뜜니다.\")\n",
    "\n",
    "\n",
    "# BoVW Histogram 생성 (학습 시 사용한 bovw_vocabulary를 그대로 사용)\n",
    "valid_query_sift_descriptors_list = [des for des in query_sift_descriptors_list if des is not None]\n",
    "\n",
    "if valid_query_sift_descriptors_list:\n",
    "    query_bovw_hist = np.array(parallel_create_bovw_histograms(valid_query_sift_descriptors_list, bovw_vocabulary, n_jobs=-1))\n",
    "    temp_bovw_hist_idx = 0\n",
    "    final_query_bovw_hist = []\n",
    "    for des in query_sift_descriptors_list:\n",
    "        if des is not None:\n",
    "            final_query_bovw_hist.append(query_bovw_hist[temp_bovw_hist_idx])\n",
    "            temp_bovw_hist_idx += 1\n",
    "        else:\n",
    "            final_query_bovw_hist.append(np.zeros(bovw_vocabulary.shape[0]))\n",
    "    query_bovw_hist = np.array(final_query_bovw_hist)\n",
    "    print(f\"  - BoVW Histogram 차원: {query_bovw_hist.shape}\")\n",
    "else:\n",
    "    print(\"  ⚠️ 쿼리 이미지에서 SIFT 디스크립터가 없어 BoVW Histogram을 생성할 수 없습니다. 0 벡터로 채웁니다.\")\n",
    "    query_bovw_hist = np.zeros((len(query_df), bovw_vocabulary.shape[0]))\n",
    "\n",
    "\n",
    "print(\"✔ 쿼리 이미지 특징 추출 완료.\")\n",
    "\n",
    "# 3. 모든 특징 결합 (학습 시와 동일한 순서와 방식으로 결합)\n",
    "query_X_features_list = [\n",
    "    query_features_color,\n",
    "    query_features_lbp,\n",
    "    query_features_hog,\n",
    "    query_bovw_hist\n",
    "]\n",
    "X_query_combined = combine_features(*query_X_features_list).astype(np.float32)\n",
    "\n",
    "print(f\"✔ 쿼리 이미지 통합 특징 벡터 생성 완료. Shape: {X_query_combined.shape}\")\n",
    "\n",
    "# 4. 학습된 Faiss KNN 모델로 예측 수행\n",
    "print(\"⏳ 쿼리 이미지 예측 및 Top-N Accuracy 계산 중...\")\n",
    "\n",
    "# 쿼리 이미지의 실제 라벨 추출 (load_images_from_folder에서 'image_path'를 반환하므로 가능)\n",
    "# 각 이미지의 상위 폴더 이름이 라벨이라고 가정합니다.\n",
    "query_true_labels = [os.path.basename(os.path.dirname(path)) for path in query_df['image_path']]\n",
    "query_true_labels_encoded = le.transform(query_true_labels)\n",
    "\n",
    "\n",
    "# Top-10 예측 결과 가져오기 (predict_faiss_knn_topk 함수 사용)\n",
    "D, I, predicted_neighbor_labels_top10 = predict_faiss_knn_topk(\n",
    "    faiss_index, train_labels_for_pred, X_query_combined, k=10\n",
    ")\n",
    "\n",
    "# Top-1 예측 라벨 (가장 가까운 이웃의 라벨)\n",
    "predicted_labels_encoded_top1 = predicted_neighbor_labels_top10[:, 0]\n",
    "predicted_labels_top1 = le.inverse_transform(predicted_labels_encoded_top1)\n",
    "\n",
    "print(\"✔ 쿼리 이미지 예측 완료.\")\n",
    "\n",
    "\n",
    "# 5. Accuracy 계산\n",
    "print(\"\\n--- Accuracy 계산 ---\")\n",
    "\n",
    "# Top-1 Accuracy 계산\n",
    "correct_top1 = (predicted_labels_encoded_top1 == query_true_labels_encoded).sum()\n",
    "total_queries = len(query_df)\n",
    "top1_accuracy = correct_top1 / total_queries\n",
    "\n",
    "print(f\"총 쿼리 이미지 개수: {total_queries}개\")\n",
    "print(f\"Top-1 정답 개수: {correct_top1}개\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "\n",
    "task2_topk_accuracy = task2_score(query_true_labels_encoded, predicted_neighbor_labels_top10, topk=10)\n",
    "print(f\"Task2 Accuracy (Top-10 Same-Class): {task2_topk_accuracy:.4f}\")\n",
    "# --------------------------------------------------\n",
    "\n",
    "\n",
    "# 6. 예측 결과 개별 출력 (예시) - 여기서는 Top-1 결과만 출력합니다.\n",
    "print(\"\\n--- 쿼리 이미지 예측 결과 (Top-1) ---\")\n",
    "import os # os 모듈이 임포트 되어있지 않을 경우를 대비\n",
    "import matplotlib.pyplot as plt # plt도 임포트되어 있는지 확인\n",
    "for i, pred_label in enumerate(predicted_labels_top1):\n",
    "    image_identifier = os.path.basename(query_df['image_path'].iloc[i]) # 파일 이름으로 식별\n",
    "    true_label_name = le.inverse_transform([query_true_labels_encoded[i]])[0]\n",
    "    print(f\"쿼리 '{image_identifier}': 실제 라벨 = {true_label_name}, 예측 라벨 (Top-1) = {pred_label}\")\n",
    "    \n",
    "    # 이미지 시각화 (선택 사항)\n",
    "    #plt.imshow(cv2.cvtColor(query_df['image_data'].iloc[i], cv2.COLOR_BGR2RGB))\n",
    "    #plt.title(f\"Predicted (Top-1): {pred_label}\\nTrue: {true_label_name}\")\n",
    "    #plt.axis('off')\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
